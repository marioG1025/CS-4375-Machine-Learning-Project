{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otbMVDhVV-9T"
   },
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "M5Xeuur4V6rl",
    "outputId": "8bff4ad2-85b1-4b67-b3cb-f6b84f1d571c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Load and Clean Steam Data\n",
    "steam_data = pd.read_csv(\"Datasets/steam.csv\")\n",
    "steam_data = steam_data.dropna(subset=['review_text'])  # Drop rows with missing reviews\n",
    "steam_data = steam_data[['review_text', 'review_score']]  # Keep only relevant columns\n",
    "steam_data = steam_data.rename(columns={'review_text': 'review', 'review_score': 'sentiment'})\n",
    "steam_data['sentiment'] = steam_data['sentiment'].apply(lambda x: 'positive' if x > 0 else 'negative')\n",
    "\n",
    "# Load and Clean Yelp Data\n",
    "yelp_data = pd.read_csv(\"Datasets/yelp.csv\", header=None)  # Load Yelp Data without a header\n",
    "yelp_data = yelp_data.iloc[:, :2]  # Select the first two columns\n",
    "yelp_data.columns = ['review', 'sentiment']  # Rename columns\n",
    "yelp_data['sentiment'] = pd.to_numeric(yelp_data['sentiment'], errors='coerce')  # Convert sentiment to numeric\n",
    "valid_sentiments = {0: 'negative', 1: 'positive'}  # Map sentiment values\n",
    "yelp_data['sentiment'] = yelp_data['sentiment'].map(valid_sentiments)\n",
    "yelp_data = yelp_data.dropna(subset=['review', 'sentiment'])  # Drop rows with missing or invalid values\n",
    "\n",
    "# Load and Clean IMDb Data\n",
    "imdb_data = pd.read_csv(\"Datasets/imdb.csv\")\n",
    "imdb_data = imdb_data[['review', 'sentiment']]  # Keep only relevant columns\n",
    "\n",
    "# Load and Clean Amazon Data\n",
    "amazon_data = pd.read_csv(\"Datasets/amazon.csv\")\n",
    "amazon_data = amazon_data[['Text', 'Score']]  # Keep only relevant columns\n",
    "amazon_data = amazon_data.rename(columns={'Text': 'review'})  # Rename columns\n",
    "amazon_data['sentiment'] = amazon_data['Score'].apply(lambda x: 'positive' if x > 3 else 'negative')\n",
    "amazon_data = amazon_data.drop(columns=['Score'])  # Drop Score after processing\n",
    "\n",
    "# Save Cleaned Datasets\n",
    "steam_data.to_csv(\"Datasets/cleaned_steam.csv\", index=False)\n",
    "yelp_data.to_csv(\"Datasets/cleaned_yelp.csv\", index=False)\n",
    "imdb_data.to_csv(\"Datasets/cleaned_imdb.csv\", index=False)\n",
    "amazon_data.to_csv(\"Datasets/cleaned_amazon.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWrxo9-BWGl_"
   },
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GX-82kUqeXyY",
    "outputId": "d93b71e4-6157-4759-c548-7c984d0d9016"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KAiVf0HL7cV1",
    "outputId": "e984f527-3ff3-4977-b83c-64c1ecc6dd1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cleaned Steam data...\n",
      "Tokenizing Steam data with NLTK...\n",
      "Starting tokenization for 6409801 rows with NLTK...\n",
      "Tokenizing row 1 of 6409801...\n",
      "Tokenizing row 1001 of 6409801...\n",
      "Tokenizing row 2001 of 6409801...\n",
      "Tokenizing row 3001 of 6409801...\n",
      "Tokenizing row 4001 of 6409801...\n",
      "Tokenizing row 5001 of 6409801...\n",
      "Tokenizing row 6001 of 6409801...\n",
      "Tokenizing row 7001 of 6409801...\n",
      "Tokenizing row 8001 of 6409801...\n",
      "Tokenizing row 9001 of 6409801...\n",
      "Tokenizing row 10001 of 6409801...\n",
      "Tokenizing row 11001 of 6409801...\n",
      "Tokenizing row 12001 of 6409801...\n",
      "Tokenizing row 13001 of 6409801...\n",
      "Tokenizing row 14001 of 6409801...\n",
      "Tokenizing row 15001 of 6409801...\n",
      "Tokenizing row 16001 of 6409801...\n",
      "Tokenizing row 17001 of 6409801...\n",
      "Tokenizing row 18001 of 6409801...\n",
      "Tokenizing row 19001 of 6409801...\n",
      "Tokenizing row 20001 of 6409801...\n",
      "Tokenizing row 21001 of 6409801...\n",
      "Tokenizing row 22001 of 6409801...\n",
      "Tokenizing row 23001 of 6409801...\n",
      "Tokenizing row 24001 of 6409801...\n",
      "Tokenizing row 25001 of 6409801...\n",
      "Tokenizing row 26001 of 6409801...\n",
      "Tokenizing row 27001 of 6409801...\n",
      "Tokenizing row 28001 of 6409801...\n",
      "Tokenizing row 29001 of 6409801...\n",
      "Tokenizing row 30001 of 6409801...\n",
      "Tokenizing row 31001 of 6409801...\n",
      "Tokenizing row 32001 of 6409801...\n",
      "Tokenizing row 33001 of 6409801...\n",
      "Tokenizing row 34001 of 6409801...\n",
      "Tokenizing row 35001 of 6409801...\n",
      "Tokenizing row 36001 of 6409801...\n",
      "Tokenizing row 37001 of 6409801...\n",
      "Tokenizing row 38001 of 6409801...\n",
      "Tokenizing row 39001 of 6409801...\n",
      "Tokenizing row 40001 of 6409801...\n",
      "Tokenizing row 41001 of 6409801...\n",
      "Tokenizing row 42001 of 6409801...\n",
      "Tokenizing row 43001 of 6409801...\n",
      "Tokenizing row 44001 of 6409801...\n",
      "Tokenizing row 45001 of 6409801...\n",
      "Tokenizing row 46001 of 6409801...\n",
      "Tokenizing row 47001 of 6409801...\n",
      "Tokenizing row 48001 of 6409801...\n",
      "Tokenizing row 49001 of 6409801...\n",
      "Tokenizing row 50001 of 6409801...\n",
      "Tokenizing row 51001 of 6409801...\n",
      "Tokenizing row 52001 of 6409801...\n",
      "Tokenizing row 53001 of 6409801...\n",
      "Tokenizing row 54001 of 6409801...\n",
      "Tokenizing row 55001 of 6409801...\n",
      "Tokenizing row 56001 of 6409801...\n",
      "Tokenizing row 57001 of 6409801...\n",
      "Tokenizing row 58001 of 6409801...\n",
      "Tokenizing row 59001 of 6409801...\n",
      "Tokenizing row 60001 of 6409801...\n",
      "Tokenizing row 61001 of 6409801...\n",
      "Tokenizing row 62001 of 6409801...\n",
      "Tokenizing row 63001 of 6409801...\n",
      "Tokenizing row 64001 of 6409801...\n",
      "Tokenizing row 65001 of 6409801...\n",
      "Tokenizing row 66001 of 6409801...\n",
      "Tokenizing row 67001 of 6409801...\n",
      "Tokenizing row 68001 of 6409801...\n",
      "Tokenizing row 69001 of 6409801...\n",
      "Tokenizing row 70001 of 6409801...\n",
      "Tokenizing row 71001 of 6409801...\n",
      "Tokenizing row 72001 of 6409801...\n",
      "Tokenizing row 73001 of 6409801...\n",
      "Tokenizing row 74001 of 6409801...\n",
      "Tokenizing row 75001 of 6409801...\n",
      "Tokenizing row 76001 of 6409801...\n",
      "Tokenizing row 77001 of 6409801...\n",
      "Tokenizing row 78001 of 6409801...\n",
      "Tokenizing row 79001 of 6409801...\n",
      "Tokenizing row 80001 of 6409801...\n",
      "Tokenizing row 81001 of 6409801...\n",
      "Tokenizing row 82001 of 6409801...\n",
      "Tokenizing row 83001 of 6409801...\n",
      "Tokenizing row 84001 of 6409801...\n",
      "Tokenizing row 85001 of 6409801...\n",
      "Tokenizing row 86001 of 6409801...\n",
      "Tokenizing row 87001 of 6409801...\n",
      "Tokenizing row 88001 of 6409801...\n",
      "Tokenizing row 89001 of 6409801...\n",
      "Tokenizing row 90001 of 6409801...\n",
      "Tokenizing row 91001 of 6409801...\n",
      "Tokenizing row 92001 of 6409801...\n",
      "Tokenizing row 93001 of 6409801...\n",
      "Tokenizing row 94001 of 6409801...\n",
      "Tokenizing row 95001 of 6409801...\n",
      "Tokenizing row 96001 of 6409801...\n",
      "Tokenizing row 97001 of 6409801...\n",
      "Tokenizing row 98001 of 6409801...\n",
      "Tokenizing row 99001 of 6409801...\n",
      "Tokenizing row 100001 of 6409801...\n",
      "Tokenizing row 101001 of 6409801...\n",
      "Tokenizing row 102001 of 6409801...\n",
      "Tokenizing row 103001 of 6409801...\n",
      "Tokenizing row 104001 of 6409801...\n",
      "Tokenizing row 105001 of 6409801...\n",
      "Tokenizing row 106001 of 6409801...\n",
      "Tokenizing row 107001 of 6409801...\n",
      "Tokenizing row 108001 of 6409801...\n",
      "Tokenizing row 109001 of 6409801...\n",
      "Tokenizing row 110001 of 6409801...\n",
      "Tokenizing row 111001 of 6409801...\n",
      "Tokenizing row 112001 of 6409801...\n",
      "Tokenizing row 113001 of 6409801...\n",
      "Tokenizing row 114001 of 6409801...\n",
      "Tokenizing row 115001 of 6409801...\n",
      "Tokenizing row 116001 of 6409801...\n",
      "Tokenizing row 117001 of 6409801...\n",
      "Tokenizing row 118001 of 6409801...\n",
      "Tokenizing row 119001 of 6409801...\n",
      "Tokenizing row 120001 of 6409801...\n",
      "Tokenizing row 121001 of 6409801...\n",
      "Tokenizing row 122001 of 6409801...\n",
      "Tokenizing row 123001 of 6409801...\n",
      "Tokenizing row 124001 of 6409801...\n",
      "Tokenizing row 125001 of 6409801...\n",
      "Tokenizing row 126001 of 6409801...\n",
      "Tokenizing row 127001 of 6409801...\n",
      "Tokenizing row 128001 of 6409801...\n",
      "Tokenizing row 129001 of 6409801...\n",
      "Tokenizing row 130001 of 6409801...\n",
      "Tokenizing row 131001 of 6409801...\n",
      "Tokenizing row 132001 of 6409801...\n",
      "Tokenizing row 133001 of 6409801...\n",
      "Tokenizing row 134001 of 6409801...\n",
      "Tokenizing row 135001 of 6409801...\n",
      "Tokenizing row 136001 of 6409801...\n",
      "Tokenizing row 137001 of 6409801...\n",
      "Tokenizing row 138001 of 6409801...\n",
      "Tokenizing row 139001 of 6409801...\n",
      "Tokenizing row 140001 of 6409801...\n",
      "Tokenizing row 141001 of 6409801...\n",
      "Tokenizing row 142001 of 6409801...\n",
      "Tokenizing row 143001 of 6409801...\n",
      "Tokenizing row 144001 of 6409801...\n",
      "Tokenizing row 145001 of 6409801...\n",
      "Tokenizing row 146001 of 6409801...\n",
      "Tokenizing row 147001 of 6409801...\n",
      "Tokenizing row 148001 of 6409801...\n",
      "Tokenizing row 149001 of 6409801...\n",
      "Tokenizing row 150001 of 6409801...\n",
      "Tokenizing row 151001 of 6409801...\n",
      "Tokenizing row 152001 of 6409801...\n",
      "Tokenizing row 153001 of 6409801...\n",
      "Tokenizing row 154001 of 6409801...\n",
      "Tokenizing row 155001 of 6409801...\n",
      "Tokenizing row 156001 of 6409801...\n",
      "Tokenizing row 157001 of 6409801...\n",
      "Tokenizing row 158001 of 6409801...\n",
      "Tokenizing row 159001 of 6409801...\n",
      "Tokenizing row 160001 of 6409801...\n",
      "Tokenizing row 161001 of 6409801...\n",
      "Tokenizing row 162001 of 6409801...\n",
      "Tokenizing row 163001 of 6409801...\n",
      "Tokenizing row 164001 of 6409801...\n",
      "Tokenizing row 165001 of 6409801...\n",
      "Tokenizing row 166001 of 6409801...\n",
      "Tokenizing row 167001 of 6409801...\n",
      "Tokenizing row 168001 of 6409801...\n",
      "Tokenizing row 169001 of 6409801...\n",
      "Tokenizing row 170001 of 6409801...\n",
      "Tokenizing row 171001 of 6409801...\n",
      "Tokenizing row 172001 of 6409801...\n",
      "Tokenizing row 173001 of 6409801...\n",
      "Tokenizing row 174001 of 6409801...\n",
      "Tokenizing row 175001 of 6409801...\n",
      "Tokenizing row 176001 of 6409801...\n",
      "Tokenizing row 177001 of 6409801...\n",
      "Tokenizing row 178001 of 6409801...\n",
      "Tokenizing row 179001 of 6409801...\n",
      "Tokenizing row 180001 of 6409801...\n",
      "Tokenizing row 181001 of 6409801...\n",
      "Tokenizing row 182001 of 6409801...\n",
      "Tokenizing row 183001 of 6409801...\n",
      "Tokenizing row 184001 of 6409801...\n",
      "Tokenizing row 185001 of 6409801...\n",
      "Tokenizing row 186001 of 6409801...\n",
      "Tokenizing row 187001 of 6409801...\n",
      "Tokenizing row 188001 of 6409801...\n",
      "Tokenizing row 189001 of 6409801...\n",
      "Tokenizing row 190001 of 6409801...\n",
      "Tokenizing row 191001 of 6409801...\n",
      "Tokenizing row 192001 of 6409801...\n",
      "Tokenizing row 193001 of 6409801...\n",
      "Tokenizing row 194001 of 6409801...\n",
      "Tokenizing row 195001 of 6409801...\n",
      "Tokenizing row 196001 of 6409801...\n",
      "Tokenizing row 197001 of 6409801...\n",
      "Tokenizing row 198001 of 6409801...\n",
      "Tokenizing row 199001 of 6409801...\n",
      "Tokenizing row 200001 of 6409801...\n",
      "Tokenizing row 201001 of 6409801...\n",
      "Tokenizing row 202001 of 6409801...\n",
      "Tokenizing row 203001 of 6409801...\n",
      "Tokenizing row 204001 of 6409801...\n",
      "Tokenizing row 205001 of 6409801...\n",
      "Tokenizing row 206001 of 6409801...\n",
      "Tokenizing row 207001 of 6409801...\n",
      "Tokenizing row 208001 of 6409801...\n",
      "Tokenizing row 209001 of 6409801...\n",
      "Tokenizing row 210001 of 6409801...\n",
      "Tokenizing row 211001 of 6409801...\n",
      "Tokenizing row 212001 of 6409801...\n",
      "Tokenizing row 213001 of 6409801...\n",
      "Tokenizing row 214001 of 6409801...\n",
      "Tokenizing row 215001 of 6409801...\n",
      "Tokenizing row 216001 of 6409801...\n",
      "Tokenizing row 217001 of 6409801...\n",
      "Tokenizing row 218001 of 6409801...\n",
      "Tokenizing row 219001 of 6409801...\n",
      "Tokenizing row 220001 of 6409801...\n",
      "Tokenizing row 221001 of 6409801...\n",
      "Tokenizing row 222001 of 6409801...\n",
      "Tokenizing row 223001 of 6409801...\n",
      "Tokenizing row 224001 of 6409801...\n",
      "Tokenizing row 225001 of 6409801...\n",
      "Tokenizing row 226001 of 6409801...\n",
      "Tokenizing row 227001 of 6409801...\n",
      "Tokenizing row 228001 of 6409801...\n",
      "Tokenizing row 229001 of 6409801...\n",
      "Tokenizing row 230001 of 6409801...\n",
      "Tokenizing row 231001 of 6409801...\n",
      "Tokenizing row 232001 of 6409801...\n",
      "Tokenizing row 233001 of 6409801...\n",
      "Tokenizing row 234001 of 6409801...\n",
      "Tokenizing row 235001 of 6409801...\n",
      "Tokenizing row 236001 of 6409801...\n",
      "Tokenizing row 237001 of 6409801...\n",
      "Tokenizing row 238001 of 6409801...\n",
      "Tokenizing row 239001 of 6409801...\n",
      "Tokenizing row 240001 of 6409801...\n",
      "Tokenizing row 241001 of 6409801...\n",
      "Tokenizing row 242001 of 6409801...\n",
      "Tokenizing row 243001 of 6409801...\n",
      "Tokenizing row 244001 of 6409801...\n",
      "Tokenizing row 245001 of 6409801...\n",
      "Tokenizing row 246001 of 6409801...\n",
      "Tokenizing row 247001 of 6409801...\n",
      "Tokenizing row 248001 of 6409801...\n",
      "Tokenizing row 249001 of 6409801...\n",
      "Tokenizing row 250001 of 6409801...\n",
      "Tokenizing row 251001 of 6409801...\n",
      "Tokenizing row 252001 of 6409801...\n",
      "Tokenizing row 253001 of 6409801...\n",
      "Tokenizing row 254001 of 6409801...\n",
      "Tokenizing row 255001 of 6409801...\n",
      "Tokenizing row 256001 of 6409801...\n",
      "Tokenizing row 257001 of 6409801...\n",
      "Tokenizing row 258001 of 6409801...\n",
      "Tokenizing row 259001 of 6409801...\n",
      "Tokenizing row 260001 of 6409801...\n",
      "Tokenizing row 261001 of 6409801...\n",
      "Tokenizing row 262001 of 6409801...\n",
      "Tokenizing row 263001 of 6409801...\n",
      "Tokenizing row 264001 of 6409801...\n",
      "Tokenizing row 265001 of 6409801...\n",
      "Tokenizing row 266001 of 6409801...\n",
      "Tokenizing row 267001 of 6409801...\n",
      "Tokenizing row 268001 of 6409801...\n",
      "Tokenizing row 269001 of 6409801...\n",
      "Tokenizing row 270001 of 6409801...\n",
      "Tokenizing row 271001 of 6409801...\n",
      "Tokenizing row 272001 of 6409801...\n",
      "Tokenizing row 273001 of 6409801...\n",
      "Tokenizing row 274001 of 6409801...\n",
      "Tokenizing row 275001 of 6409801...\n",
      "Tokenizing row 276001 of 6409801...\n",
      "Tokenizing row 277001 of 6409801...\n",
      "Tokenizing row 278001 of 6409801...\n",
      "Tokenizing row 279001 of 6409801...\n",
      "Tokenizing row 280001 of 6409801...\n",
      "Tokenizing row 281001 of 6409801...\n",
      "Tokenizing row 282001 of 6409801...\n",
      "Tokenizing row 283001 of 6409801...\n",
      "Tokenizing row 284001 of 6409801...\n",
      "Tokenizing row 285001 of 6409801...\n",
      "Tokenizing row 286001 of 6409801...\n",
      "Tokenizing row 287001 of 6409801...\n",
      "Tokenizing row 288001 of 6409801...\n",
      "Tokenizing row 289001 of 6409801...\n",
      "Tokenizing row 290001 of 6409801...\n",
      "Tokenizing row 291001 of 6409801...\n",
      "Tokenizing row 292001 of 6409801...\n",
      "Tokenizing row 293001 of 6409801...\n",
      "Tokenizing row 294001 of 6409801...\n",
      "Tokenizing row 295001 of 6409801...\n",
      "Tokenizing row 296001 of 6409801...\n",
      "Tokenizing row 297001 of 6409801...\n",
      "Tokenizing row 298001 of 6409801...\n",
      "Tokenizing row 299001 of 6409801...\n",
      "Tokenizing row 300001 of 6409801...\n",
      "Tokenizing row 301001 of 6409801...\n",
      "Tokenizing row 302001 of 6409801...\n",
      "Tokenizing row 303001 of 6409801...\n",
      "Tokenizing row 304001 of 6409801...\n",
      "Tokenizing row 305001 of 6409801...\n",
      "Tokenizing row 306001 of 6409801...\n",
      "Tokenizing row 307001 of 6409801...\n",
      "Tokenizing row 308001 of 6409801...\n",
      "Tokenizing row 309001 of 6409801...\n",
      "Tokenizing row 310001 of 6409801...\n",
      "Tokenizing row 311001 of 6409801...\n",
      "Tokenizing row 312001 of 6409801...\n",
      "Tokenizing row 313001 of 6409801...\n",
      "Tokenizing row 314001 of 6409801...\n",
      "Tokenizing row 315001 of 6409801...\n",
      "Tokenizing row 316001 of 6409801...\n",
      "Tokenizing row 317001 of 6409801...\n",
      "Tokenizing row 318001 of 6409801...\n",
      "Tokenizing row 319001 of 6409801...\n",
      "Tokenizing row 320001 of 6409801...\n",
      "Tokenizing row 321001 of 6409801...\n",
      "Tokenizing row 322001 of 6409801...\n",
      "Tokenizing row 323001 of 6409801...\n",
      "Tokenizing row 324001 of 6409801...\n",
      "Tokenizing row 325001 of 6409801...\n",
      "Tokenizing row 326001 of 6409801...\n",
      "Tokenizing row 327001 of 6409801...\n",
      "Tokenizing row 328001 of 6409801...\n",
      "Tokenizing row 329001 of 6409801...\n",
      "Tokenizing row 330001 of 6409801...\n",
      "Tokenizing row 331001 of 6409801...\n",
      "Tokenizing row 332001 of 6409801...\n",
      "Tokenizing row 333001 of 6409801...\n",
      "Tokenizing row 334001 of 6409801...\n",
      "Tokenizing row 335001 of 6409801...\n",
      "Tokenizing row 336001 of 6409801...\n",
      "Tokenizing row 337001 of 6409801...\n",
      "Tokenizing row 338001 of 6409801...\n",
      "Tokenizing row 339001 of 6409801...\n",
      "Tokenizing row 340001 of 6409801...\n",
      "Tokenizing row 341001 of 6409801...\n",
      "Tokenizing row 342001 of 6409801...\n",
      "Tokenizing row 343001 of 6409801...\n",
      "Tokenizing row 344001 of 6409801...\n",
      "Tokenizing row 345001 of 6409801...\n",
      "Tokenizing row 346001 of 6409801...\n",
      "Tokenizing row 347001 of 6409801...\n",
      "Tokenizing row 348001 of 6409801...\n",
      "Tokenizing row 349001 of 6409801...\n",
      "Tokenizing row 350001 of 6409801...\n",
      "Tokenizing row 351001 of 6409801...\n",
      "Tokenizing row 352001 of 6409801...\n",
      "Tokenizing row 353001 of 6409801...\n",
      "Tokenizing row 354001 of 6409801...\n",
      "Tokenizing row 355001 of 6409801...\n",
      "Tokenizing row 356001 of 6409801...\n",
      "Tokenizing row 357001 of 6409801...\n",
      "Tokenizing row 358001 of 6409801...\n",
      "Tokenizing row 359001 of 6409801...\n",
      "Tokenizing row 360001 of 6409801...\n",
      "Tokenizing row 361001 of 6409801...\n",
      "Tokenizing row 362001 of 6409801...\n",
      "Tokenizing row 363001 of 6409801...\n",
      "Tokenizing row 364001 of 6409801...\n",
      "Tokenizing row 365001 of 6409801...\n",
      "Tokenizing row 366001 of 6409801...\n",
      "Tokenizing row 367001 of 6409801...\n",
      "Tokenizing row 368001 of 6409801...\n",
      "Tokenizing row 369001 of 6409801...\n",
      "Tokenizing row 370001 of 6409801...\n",
      "Tokenizing row 371001 of 6409801...\n",
      "Tokenizing row 372001 of 6409801...\n",
      "Tokenizing row 373001 of 6409801...\n",
      "Tokenizing row 374001 of 6409801...\n",
      "Tokenizing row 375001 of 6409801...\n",
      "Tokenizing row 376001 of 6409801...\n",
      "Tokenizing row 377001 of 6409801...\n",
      "Tokenizing row 378001 of 6409801...\n",
      "Tokenizing row 379001 of 6409801...\n",
      "Tokenizing row 380001 of 6409801...\n",
      "Tokenizing row 381001 of 6409801...\n",
      "Tokenizing row 382001 of 6409801...\n",
      "Tokenizing row 383001 of 6409801...\n",
      "Tokenizing row 384001 of 6409801...\n",
      "Tokenizing row 385001 of 6409801...\n",
      "Tokenizing row 386001 of 6409801...\n",
      "Tokenizing row 387001 of 6409801...\n",
      "Tokenizing row 388001 of 6409801...\n",
      "Tokenizing row 389001 of 6409801...\n",
      "Tokenizing row 390001 of 6409801...\n",
      "Tokenizing row 391001 of 6409801...\n",
      "Tokenizing row 392001 of 6409801...\n",
      "Tokenizing row 393001 of 6409801...\n",
      "Tokenizing row 394001 of 6409801...\n",
      "Tokenizing row 395001 of 6409801...\n",
      "Tokenizing row 396001 of 6409801...\n",
      "Tokenizing row 397001 of 6409801...\n",
      "Tokenizing row 398001 of 6409801...\n",
      "Tokenizing row 399001 of 6409801...\n",
      "Tokenizing row 400001 of 6409801...\n",
      "Tokenizing row 401001 of 6409801...\n",
      "Tokenizing row 402001 of 6409801...\n",
      "Tokenizing row 403001 of 6409801...\n",
      "Tokenizing row 404001 of 6409801...\n",
      "Tokenizing row 405001 of 6409801...\n",
      "Tokenizing row 406001 of 6409801...\n",
      "Tokenizing row 407001 of 6409801...\n",
      "Tokenizing row 408001 of 6409801...\n",
      "Tokenizing row 409001 of 6409801...\n",
      "Tokenizing row 410001 of 6409801...\n",
      "Tokenizing row 411001 of 6409801...\n",
      "Tokenizing row 412001 of 6409801...\n",
      "Tokenizing row 413001 of 6409801...\n",
      "Tokenizing row 414001 of 6409801...\n",
      "Tokenizing row 415001 of 6409801...\n",
      "Tokenizing row 416001 of 6409801...\n",
      "Tokenizing row 417001 of 6409801...\n",
      "Tokenizing row 418001 of 6409801...\n",
      "Tokenizing row 419001 of 6409801...\n",
      "Tokenizing row 420001 of 6409801...\n",
      "Tokenizing row 421001 of 6409801...\n",
      "Tokenizing row 422001 of 6409801...\n",
      "Tokenizing row 423001 of 6409801...\n",
      "Tokenizing row 424001 of 6409801...\n",
      "Tokenizing row 425001 of 6409801...\n",
      "Tokenizing row 426001 of 6409801...\n",
      "Tokenizing row 427001 of 6409801...\n",
      "Tokenizing row 428001 of 6409801...\n",
      "Tokenizing row 429001 of 6409801...\n",
      "Tokenizing row 430001 of 6409801...\n",
      "Tokenizing row 431001 of 6409801...\n",
      "Tokenizing row 432001 of 6409801...\n",
      "Tokenizing row 433001 of 6409801...\n",
      "Tokenizing row 434001 of 6409801...\n",
      "Tokenizing row 435001 of 6409801...\n",
      "Tokenizing row 436001 of 6409801...\n",
      "Tokenizing row 437001 of 6409801...\n",
      "Tokenizing row 438001 of 6409801...\n",
      "Tokenizing row 439001 of 6409801...\n",
      "Tokenizing row 440001 of 6409801...\n",
      "Tokenizing row 441001 of 6409801...\n",
      "Tokenizing row 442001 of 6409801...\n",
      "Tokenizing row 443001 of 6409801...\n",
      "Tokenizing row 444001 of 6409801...\n",
      "Tokenizing row 445001 of 6409801...\n",
      "Tokenizing row 446001 of 6409801...\n",
      "Tokenizing row 447001 of 6409801...\n",
      "Tokenizing row 448001 of 6409801...\n",
      "Tokenizing row 449001 of 6409801...\n",
      "Tokenizing row 450001 of 6409801...\n",
      "Tokenizing row 451001 of 6409801...\n",
      "Tokenizing row 452001 of 6409801...\n",
      "Tokenizing row 453001 of 6409801...\n",
      "Tokenizing row 454001 of 6409801...\n",
      "Tokenizing row 455001 of 6409801...\n",
      "Tokenizing row 456001 of 6409801...\n",
      "Tokenizing row 457001 of 6409801...\n",
      "Tokenizing row 458001 of 6409801...\n",
      "Tokenizing row 459001 of 6409801...\n",
      "Tokenizing row 460001 of 6409801...\n",
      "Tokenizing row 461001 of 6409801...\n",
      "Tokenizing row 462001 of 6409801...\n",
      "Tokenizing row 463001 of 6409801...\n",
      "Tokenizing row 464001 of 6409801...\n",
      "Tokenizing row 465001 of 6409801...\n",
      "Tokenizing row 466001 of 6409801...\n",
      "Tokenizing row 467001 of 6409801...\n",
      "Tokenizing row 468001 of 6409801...\n",
      "Tokenizing row 469001 of 6409801...\n",
      "Tokenizing row 470001 of 6409801...\n",
      "Tokenizing row 471001 of 6409801...\n",
      "Tokenizing row 472001 of 6409801...\n",
      "Tokenizing row 473001 of 6409801...\n",
      "Tokenizing row 474001 of 6409801...\n",
      "Tokenizing row 475001 of 6409801...\n",
      "Tokenizing row 476001 of 6409801...\n",
      "Tokenizing row 477001 of 6409801...\n",
      "Tokenizing row 478001 of 6409801...\n",
      "Tokenizing row 479001 of 6409801...\n",
      "Tokenizing row 480001 of 6409801...\n",
      "Tokenizing row 481001 of 6409801...\n",
      "Tokenizing row 482001 of 6409801...\n",
      "Tokenizing row 483001 of 6409801...\n",
      "Tokenizing row 484001 of 6409801...\n",
      "Tokenizing row 485001 of 6409801...\n",
      "Tokenizing row 486001 of 6409801...\n",
      "Tokenizing row 487001 of 6409801...\n",
      "Tokenizing row 488001 of 6409801...\n",
      "Tokenizing row 489001 of 6409801...\n",
      "Tokenizing row 490001 of 6409801...\n",
      "Tokenizing row 491001 of 6409801...\n",
      "Tokenizing row 492001 of 6409801...\n",
      "Tokenizing row 493001 of 6409801...\n",
      "Tokenizing row 494001 of 6409801...\n",
      "Tokenizing row 495001 of 6409801...\n",
      "Tokenizing row 496001 of 6409801...\n",
      "Tokenizing row 497001 of 6409801...\n",
      "Tokenizing row 498001 of 6409801...\n",
      "Tokenizing row 499001 of 6409801...\n",
      "Tokenizing row 500001 of 6409801...\n",
      "Tokenizing row 501001 of 6409801...\n",
      "Tokenizing row 502001 of 6409801...\n",
      "Tokenizing row 503001 of 6409801...\n",
      "Tokenizing row 504001 of 6409801...\n",
      "Tokenizing row 505001 of 6409801...\n",
      "Tokenizing row 506001 of 6409801...\n",
      "Tokenizing row 507001 of 6409801...\n",
      "Tokenizing row 508001 of 6409801...\n",
      "Tokenizing row 509001 of 6409801...\n",
      "Tokenizing row 510001 of 6409801...\n",
      "Tokenizing row 511001 of 6409801...\n",
      "Tokenizing row 512001 of 6409801...\n",
      "Tokenizing row 513001 of 6409801...\n",
      "Tokenizing row 514001 of 6409801...\n",
      "Tokenizing row 515001 of 6409801...\n",
      "Tokenizing row 516001 of 6409801...\n",
      "Tokenizing row 517001 of 6409801...\n",
      "Tokenizing row 518001 of 6409801...\n",
      "Tokenizing row 519001 of 6409801...\n",
      "Tokenizing row 520001 of 6409801...\n",
      "Tokenizing row 521001 of 6409801...\n",
      "Tokenizing row 522001 of 6409801...\n",
      "Tokenizing row 523001 of 6409801...\n",
      "Tokenizing row 524001 of 6409801...\n",
      "Tokenizing row 525001 of 6409801...\n",
      "Tokenizing row 526001 of 6409801...\n",
      "Tokenizing row 527001 of 6409801...\n",
      "Tokenizing row 528001 of 6409801...\n",
      "Tokenizing row 529001 of 6409801...\n",
      "Tokenizing row 530001 of 6409801...\n",
      "Tokenizing row 531001 of 6409801...\n",
      "Tokenizing row 532001 of 6409801...\n",
      "Tokenizing row 533001 of 6409801...\n",
      "Tokenizing row 534001 of 6409801...\n",
      "Tokenizing row 535001 of 6409801...\n",
      "Tokenizing row 536001 of 6409801...\n",
      "Tokenizing row 537001 of 6409801...\n",
      "Tokenizing row 538001 of 6409801...\n",
      "Tokenizing row 539001 of 6409801...\n",
      "Tokenizing row 540001 of 6409801...\n",
      "Tokenizing row 541001 of 6409801...\n",
      "Tokenizing row 542001 of 6409801...\n",
      "Tokenizing row 543001 of 6409801...\n",
      "Tokenizing row 544001 of 6409801...\n",
      "Tokenizing row 545001 of 6409801...\n",
      "Tokenizing row 546001 of 6409801...\n",
      "Tokenizing row 547001 of 6409801...\n",
      "Tokenizing row 548001 of 6409801...\n",
      "Tokenizing row 549001 of 6409801...\n",
      "Tokenizing row 550001 of 6409801...\n",
      "Tokenizing row 551001 of 6409801...\n",
      "Tokenizing row 552001 of 6409801...\n",
      "Tokenizing row 553001 of 6409801...\n",
      "Tokenizing row 554001 of 6409801...\n",
      "Tokenizing row 555001 of 6409801...\n",
      "Tokenizing row 556001 of 6409801...\n",
      "Tokenizing row 557001 of 6409801...\n",
      "Tokenizing row 558001 of 6409801...\n",
      "Tokenizing row 559001 of 6409801...\n",
      "Tokenizing row 560001 of 6409801...\n",
      "Tokenizing row 561001 of 6409801...\n",
      "Tokenizing row 562001 of 6409801...\n",
      "Tokenizing row 563001 of 6409801...\n",
      "Tokenizing row 564001 of 6409801...\n",
      "Tokenizing row 565001 of 6409801...\n",
      "Tokenizing row 566001 of 6409801...\n",
      "Tokenizing row 567001 of 6409801...\n",
      "Tokenizing row 568001 of 6409801...\n",
      "Tokenizing row 569001 of 6409801...\n",
      "Tokenizing row 570001 of 6409801...\n",
      "Tokenizing row 571001 of 6409801...\n",
      "Tokenizing row 572001 of 6409801...\n",
      "Tokenizing row 573001 of 6409801...\n",
      "Tokenizing row 574001 of 6409801...\n",
      "Tokenizing row 575001 of 6409801...\n",
      "Tokenizing row 576001 of 6409801...\n",
      "Tokenizing row 577001 of 6409801...\n",
      "Tokenizing row 578001 of 6409801...\n",
      "Tokenizing row 579001 of 6409801...\n",
      "Tokenizing row 580001 of 6409801...\n",
      "Tokenizing row 581001 of 6409801...\n",
      "Tokenizing row 582001 of 6409801...\n",
      "Tokenizing row 583001 of 6409801...\n",
      "Tokenizing row 584001 of 6409801...\n",
      "Tokenizing row 585001 of 6409801...\n",
      "Tokenizing row 586001 of 6409801...\n",
      "Tokenizing row 587001 of 6409801...\n",
      "Tokenizing row 588001 of 6409801...\n",
      "Tokenizing row 589001 of 6409801...\n",
      "Tokenizing row 590001 of 6409801...\n",
      "Tokenizing row 591001 of 6409801...\n",
      "Tokenizing row 592001 of 6409801...\n",
      "Tokenizing row 593001 of 6409801...\n",
      "Tokenizing row 594001 of 6409801...\n",
      "Tokenizing row 595001 of 6409801...\n",
      "Tokenizing row 596001 of 6409801...\n",
      "Tokenizing row 597001 of 6409801...\n",
      "Tokenizing row 598001 of 6409801...\n",
      "Tokenizing row 599001 of 6409801...\n",
      "Tokenizing row 600001 of 6409801...\n",
      "Tokenizing row 601001 of 6409801...\n",
      "Tokenizing row 602001 of 6409801...\n",
      "Tokenizing row 603001 of 6409801...\n",
      "Tokenizing row 604001 of 6409801...\n",
      "Tokenizing row 605001 of 6409801...\n",
      "Tokenizing row 606001 of 6409801...\n",
      "Tokenizing row 607001 of 6409801...\n",
      "Tokenizing row 608001 of 6409801...\n",
      "Tokenizing row 609001 of 6409801...\n",
      "Tokenizing row 610001 of 6409801...\n",
      "Tokenizing row 611001 of 6409801...\n",
      "Tokenizing row 612001 of 6409801...\n",
      "Tokenizing row 613001 of 6409801...\n",
      "Tokenizing row 614001 of 6409801...\n",
      "Tokenizing row 615001 of 6409801...\n",
      "Tokenizing row 616001 of 6409801...\n",
      "Tokenizing row 617001 of 6409801...\n",
      "Tokenizing row 618001 of 6409801...\n",
      "Tokenizing row 619001 of 6409801...\n",
      "Tokenizing row 620001 of 6409801...\n",
      "Tokenizing row 621001 of 6409801...\n",
      "Tokenizing row 622001 of 6409801...\n",
      "Tokenizing row 623001 of 6409801...\n",
      "Tokenizing row 624001 of 6409801...\n",
      "Tokenizing row 625001 of 6409801...\n",
      "Tokenizing row 626001 of 6409801...\n",
      "Tokenizing row 627001 of 6409801...\n",
      "Tokenizing row 628001 of 6409801...\n",
      "Tokenizing row 629001 of 6409801...\n",
      "Tokenizing row 630001 of 6409801...\n",
      "Tokenizing row 631001 of 6409801...\n",
      "Tokenizing row 632001 of 6409801...\n",
      "Tokenizing row 633001 of 6409801...\n",
      "Tokenizing row 634001 of 6409801...\n",
      "Tokenizing row 635001 of 6409801...\n",
      "Tokenizing row 636001 of 6409801...\n",
      "Tokenizing row 637001 of 6409801...\n",
      "Tokenizing row 638001 of 6409801...\n",
      "Tokenizing row 639001 of 6409801...\n",
      "Tokenizing row 640001 of 6409801...\n",
      "Tokenizing row 641001 of 6409801...\n",
      "Tokenizing row 642001 of 6409801...\n",
      "Tokenizing row 643001 of 6409801...\n",
      "Tokenizing row 644001 of 6409801...\n",
      "Tokenizing row 645001 of 6409801...\n",
      "Tokenizing row 646001 of 6409801...\n",
      "Tokenizing row 647001 of 6409801...\n",
      "Tokenizing row 648001 of 6409801...\n",
      "Tokenizing row 649001 of 6409801...\n",
      "Tokenizing row 650001 of 6409801...\n",
      "Tokenizing row 651001 of 6409801...\n",
      "Tokenizing row 652001 of 6409801...\n",
      "Tokenizing row 653001 of 6409801...\n",
      "Tokenizing row 654001 of 6409801...\n",
      "Tokenizing row 655001 of 6409801...\n",
      "Tokenizing row 656001 of 6409801...\n",
      "Tokenizing row 657001 of 6409801...\n",
      "Tokenizing row 658001 of 6409801...\n",
      "Tokenizing row 659001 of 6409801...\n",
      "Tokenizing row 660001 of 6409801...\n",
      "Tokenizing row 661001 of 6409801...\n",
      "Tokenizing row 662001 of 6409801...\n",
      "Tokenizing row 663001 of 6409801...\n",
      "Tokenizing row 664001 of 6409801...\n",
      "Tokenizing row 665001 of 6409801...\n",
      "Tokenizing row 666001 of 6409801...\n",
      "Tokenizing row 667001 of 6409801...\n",
      "Tokenizing row 668001 of 6409801...\n",
      "Tokenizing row 669001 of 6409801...\n",
      "Tokenizing row 670001 of 6409801...\n",
      "Tokenizing row 671001 of 6409801...\n",
      "Tokenizing row 672001 of 6409801...\n",
      "Tokenizing row 673001 of 6409801...\n",
      "Tokenizing row 674001 of 6409801...\n",
      "Tokenizing row 675001 of 6409801...\n",
      "Tokenizing row 676001 of 6409801...\n",
      "Tokenizing row 677001 of 6409801...\n",
      "Tokenizing row 678001 of 6409801...\n",
      "Tokenizing row 679001 of 6409801...\n",
      "Tokenizing row 680001 of 6409801...\n",
      "Tokenizing row 681001 of 6409801...\n",
      "Tokenizing row 682001 of 6409801...\n",
      "Tokenizing row 683001 of 6409801...\n",
      "Tokenizing row 684001 of 6409801...\n",
      "Tokenizing row 685001 of 6409801...\n",
      "Tokenizing row 686001 of 6409801...\n",
      "Tokenizing row 687001 of 6409801...\n",
      "Tokenizing row 688001 of 6409801...\n",
      "Tokenizing row 689001 of 6409801...\n",
      "Tokenizing row 690001 of 6409801...\n",
      "Tokenizing row 691001 of 6409801...\n",
      "Tokenizing row 692001 of 6409801...\n",
      "Tokenizing row 693001 of 6409801...\n",
      "Tokenizing row 694001 of 6409801...\n",
      "Tokenizing row 695001 of 6409801...\n",
      "Tokenizing row 696001 of 6409801...\n",
      "Tokenizing row 697001 of 6409801...\n",
      "Tokenizing row 698001 of 6409801...\n",
      "Tokenizing row 699001 of 6409801...\n",
      "Tokenizing row 700001 of 6409801...\n",
      "Tokenizing row 701001 of 6409801...\n",
      "Tokenizing row 702001 of 6409801...\n",
      "Tokenizing row 703001 of 6409801...\n",
      "Tokenizing row 704001 of 6409801...\n",
      "Tokenizing row 705001 of 6409801...\n",
      "Tokenizing row 706001 of 6409801...\n",
      "Tokenizing row 707001 of 6409801...\n",
      "Tokenizing row 708001 of 6409801...\n",
      "Tokenizing row 709001 of 6409801...\n",
      "Tokenizing row 710001 of 6409801...\n",
      "Tokenizing row 711001 of 6409801...\n",
      "Tokenizing row 712001 of 6409801...\n",
      "Tokenizing row 713001 of 6409801...\n",
      "Tokenizing row 714001 of 6409801...\n",
      "Tokenizing row 715001 of 6409801...\n",
      "Tokenizing row 716001 of 6409801...\n",
      "Tokenizing row 717001 of 6409801...\n",
      "Tokenizing row 718001 of 6409801...\n",
      "Tokenizing row 719001 of 6409801...\n",
      "Tokenizing row 720001 of 6409801...\n",
      "Tokenizing row 721001 of 6409801...\n",
      "Tokenizing row 722001 of 6409801...\n",
      "Tokenizing row 723001 of 6409801...\n",
      "Tokenizing row 724001 of 6409801...\n",
      "Tokenizing row 725001 of 6409801...\n",
      "Tokenizing row 726001 of 6409801...\n",
      "Tokenizing row 727001 of 6409801...\n",
      "Tokenizing row 728001 of 6409801...\n",
      "Tokenizing row 729001 of 6409801...\n",
      "Tokenizing row 730001 of 6409801...\n",
      "Tokenizing row 731001 of 6409801...\n",
      "Tokenizing row 732001 of 6409801...\n",
      "Tokenizing row 733001 of 6409801...\n",
      "Tokenizing row 734001 of 6409801...\n",
      "Tokenizing row 735001 of 6409801...\n",
      "Tokenizing row 736001 of 6409801...\n",
      "Tokenizing row 737001 of 6409801...\n",
      "Tokenizing row 738001 of 6409801...\n",
      "Tokenizing row 739001 of 6409801...\n",
      "Tokenizing row 740001 of 6409801...\n",
      "Tokenizing row 741001 of 6409801...\n",
      "Tokenizing row 742001 of 6409801...\n",
      "Tokenizing row 743001 of 6409801...\n",
      "Tokenizing row 744001 of 6409801...\n",
      "Tokenizing row 745001 of 6409801...\n",
      "Tokenizing row 746001 of 6409801...\n",
      "Tokenizing row 747001 of 6409801...\n",
      "Tokenizing row 748001 of 6409801...\n",
      "Tokenizing row 749001 of 6409801...\n",
      "Tokenizing row 750001 of 6409801...\n",
      "Tokenizing row 751001 of 6409801...\n",
      "Tokenizing row 752001 of 6409801...\n",
      "Tokenizing row 753001 of 6409801...\n",
      "Tokenizing row 754001 of 6409801...\n",
      "Tokenizing row 755001 of 6409801...\n",
      "Tokenizing row 756001 of 6409801...\n",
      "Tokenizing row 757001 of 6409801...\n",
      "Tokenizing row 758001 of 6409801...\n",
      "Tokenizing row 759001 of 6409801...\n",
      "Tokenizing row 760001 of 6409801...\n",
      "Tokenizing row 761001 of 6409801...\n",
      "Tokenizing row 762001 of 6409801...\n",
      "Tokenizing row 763001 of 6409801...\n",
      "Tokenizing row 764001 of 6409801...\n",
      "Tokenizing row 765001 of 6409801...\n",
      "Tokenizing row 766001 of 6409801...\n",
      "Tokenizing row 767001 of 6409801...\n",
      "Tokenizing row 768001 of 6409801...\n",
      "Tokenizing row 769001 of 6409801...\n",
      "Tokenizing row 770001 of 6409801...\n",
      "Tokenizing row 771001 of 6409801...\n",
      "Tokenizing row 772001 of 6409801...\n",
      "Tokenizing row 773001 of 6409801...\n",
      "Tokenizing row 774001 of 6409801...\n",
      "Tokenizing row 775001 of 6409801...\n",
      "Tokenizing row 776001 of 6409801...\n",
      "Tokenizing row 777001 of 6409801...\n",
      "Tokenizing row 778001 of 6409801...\n",
      "Tokenizing row 779001 of 6409801...\n",
      "Tokenizing row 780001 of 6409801...\n",
      "Tokenizing row 781001 of 6409801...\n",
      "Tokenizing row 782001 of 6409801...\n",
      "Tokenizing row 783001 of 6409801...\n",
      "Tokenizing row 784001 of 6409801...\n",
      "Tokenizing row 785001 of 6409801...\n",
      "Tokenizing row 786001 of 6409801...\n",
      "Tokenizing row 787001 of 6409801...\n",
      "Tokenizing row 788001 of 6409801...\n",
      "Tokenizing row 789001 of 6409801...\n",
      "Tokenizing row 790001 of 6409801...\n",
      "Tokenizing row 791001 of 6409801...\n",
      "Tokenizing row 792001 of 6409801...\n",
      "Tokenizing row 793001 of 6409801...\n",
      "Tokenizing row 794001 of 6409801...\n",
      "Tokenizing row 795001 of 6409801...\n",
      "Tokenizing row 796001 of 6409801...\n",
      "Tokenizing row 797001 of 6409801...\n",
      "Tokenizing row 798001 of 6409801...\n",
      "Tokenizing row 799001 of 6409801...\n",
      "Tokenizing row 800001 of 6409801...\n",
      "Tokenizing row 801001 of 6409801...\n",
      "Tokenizing row 802001 of 6409801...\n",
      "Tokenizing row 803001 of 6409801...\n",
      "Tokenizing row 804001 of 6409801...\n",
      "Tokenizing row 805001 of 6409801...\n",
      "Tokenizing row 806001 of 6409801...\n",
      "Tokenizing row 807001 of 6409801...\n",
      "Tokenizing row 808001 of 6409801...\n",
      "Tokenizing row 809001 of 6409801...\n",
      "Tokenizing row 810001 of 6409801...\n",
      "Tokenizing row 811001 of 6409801...\n",
      "Tokenizing row 812001 of 6409801...\n",
      "Tokenizing row 813001 of 6409801...\n",
      "Tokenizing row 814001 of 6409801...\n",
      "Tokenizing row 815001 of 6409801...\n",
      "Tokenizing row 816001 of 6409801...\n",
      "Tokenizing row 817001 of 6409801...\n",
      "Tokenizing row 818001 of 6409801...\n",
      "Tokenizing row 819001 of 6409801...\n",
      "Tokenizing row 820001 of 6409801...\n",
      "Tokenizing row 821001 of 6409801...\n",
      "Tokenizing row 822001 of 6409801...\n",
      "Tokenizing row 823001 of 6409801...\n",
      "Tokenizing row 824001 of 6409801...\n",
      "Tokenizing row 825001 of 6409801...\n",
      "Tokenizing row 826001 of 6409801...\n",
      "Tokenizing row 827001 of 6409801...\n",
      "Tokenizing row 828001 of 6409801...\n",
      "Tokenizing row 829001 of 6409801...\n",
      "Tokenizing row 830001 of 6409801...\n",
      "Tokenizing row 831001 of 6409801...\n",
      "Tokenizing row 832001 of 6409801...\n",
      "Tokenizing row 833001 of 6409801...\n",
      "Tokenizing row 834001 of 6409801...\n",
      "Tokenizing row 835001 of 6409801...\n",
      "Tokenizing row 836001 of 6409801...\n",
      "Tokenizing row 837001 of 6409801...\n",
      "Tokenizing row 838001 of 6409801...\n",
      "Tokenizing row 839001 of 6409801...\n",
      "Tokenizing row 840001 of 6409801...\n",
      "Tokenizing row 841001 of 6409801...\n",
      "Tokenizing row 842001 of 6409801...\n",
      "Tokenizing row 843001 of 6409801...\n",
      "Tokenizing row 844001 of 6409801...\n",
      "Tokenizing row 845001 of 6409801...\n",
      "Tokenizing row 846001 of 6409801...\n",
      "Tokenizing row 847001 of 6409801...\n",
      "Tokenizing row 848001 of 6409801...\n",
      "Tokenizing row 849001 of 6409801...\n",
      "Tokenizing row 850001 of 6409801...\n",
      "Tokenizing row 851001 of 6409801...\n",
      "Tokenizing row 852001 of 6409801...\n",
      "Tokenizing row 853001 of 6409801...\n",
      "Tokenizing row 854001 of 6409801...\n",
      "Tokenizing row 855001 of 6409801...\n",
      "Tokenizing row 856001 of 6409801...\n",
      "Tokenizing row 857001 of 6409801...\n",
      "Tokenizing row 858001 of 6409801...\n",
      "Tokenizing row 859001 of 6409801...\n",
      "Tokenizing row 860001 of 6409801...\n",
      "Tokenizing row 861001 of 6409801...\n",
      "Tokenizing row 862001 of 6409801...\n",
      "Tokenizing row 863001 of 6409801...\n",
      "Tokenizing row 864001 of 6409801...\n",
      "Tokenizing row 865001 of 6409801...\n",
      "Tokenizing row 866001 of 6409801...\n",
      "Tokenizing row 867001 of 6409801...\n",
      "Tokenizing row 868001 of 6409801...\n",
      "Tokenizing row 869001 of 6409801...\n",
      "Tokenizing row 870001 of 6409801...\n",
      "Tokenizing row 871001 of 6409801...\n",
      "Tokenizing row 872001 of 6409801...\n",
      "Tokenizing row 873001 of 6409801...\n",
      "Tokenizing row 874001 of 6409801...\n",
      "Tokenizing row 875001 of 6409801...\n",
      "Tokenizing row 876001 of 6409801...\n",
      "Tokenizing row 877001 of 6409801...\n",
      "Tokenizing row 878001 of 6409801...\n",
      "Tokenizing row 879001 of 6409801...\n",
      "Tokenizing row 880001 of 6409801...\n",
      "Tokenizing row 881001 of 6409801...\n",
      "Tokenizing row 882001 of 6409801...\n",
      "Tokenizing row 883001 of 6409801...\n",
      "Tokenizing row 884001 of 6409801...\n",
      "Tokenizing row 885001 of 6409801...\n",
      "Tokenizing row 886001 of 6409801...\n",
      "Tokenizing row 887001 of 6409801...\n",
      "Tokenizing row 888001 of 6409801...\n",
      "Tokenizing row 889001 of 6409801...\n",
      "Tokenizing row 890001 of 6409801...\n",
      "Tokenizing row 891001 of 6409801...\n",
      "Tokenizing row 892001 of 6409801...\n",
      "Tokenizing row 893001 of 6409801...\n",
      "Tokenizing row 894001 of 6409801...\n",
      "Tokenizing row 895001 of 6409801...\n",
      "Tokenizing row 896001 of 6409801...\n",
      "Tokenizing row 897001 of 6409801...\n",
      "Tokenizing row 898001 of 6409801...\n",
      "Tokenizing row 899001 of 6409801...\n",
      "Tokenizing row 900001 of 6409801...\n",
      "Tokenizing row 901001 of 6409801...\n",
      "Tokenizing row 902001 of 6409801...\n",
      "Tokenizing row 903001 of 6409801...\n",
      "Tokenizing row 904001 of 6409801...\n",
      "Tokenizing row 905001 of 6409801...\n",
      "Tokenizing row 906001 of 6409801...\n",
      "Tokenizing row 907001 of 6409801...\n",
      "Tokenizing row 908001 of 6409801...\n",
      "Tokenizing row 909001 of 6409801...\n",
      "Tokenizing row 910001 of 6409801...\n",
      "Tokenizing row 911001 of 6409801...\n",
      "Tokenizing row 912001 of 6409801...\n",
      "Tokenizing row 913001 of 6409801...\n",
      "Tokenizing row 914001 of 6409801...\n",
      "Tokenizing row 915001 of 6409801...\n",
      "Tokenizing row 916001 of 6409801...\n",
      "Tokenizing row 917001 of 6409801...\n",
      "Tokenizing row 918001 of 6409801...\n",
      "Tokenizing row 919001 of 6409801...\n",
      "Tokenizing row 920001 of 6409801...\n",
      "Tokenizing row 921001 of 6409801...\n",
      "Tokenizing row 922001 of 6409801...\n",
      "Tokenizing row 923001 of 6409801...\n",
      "Tokenizing row 924001 of 6409801...\n",
      "Tokenizing row 925001 of 6409801...\n",
      "Tokenizing row 926001 of 6409801...\n",
      "Tokenizing row 927001 of 6409801...\n",
      "Tokenizing row 928001 of 6409801...\n",
      "Tokenizing row 929001 of 6409801...\n",
      "Tokenizing row 930001 of 6409801...\n",
      "Tokenizing row 931001 of 6409801...\n",
      "Tokenizing row 932001 of 6409801...\n",
      "Tokenizing row 933001 of 6409801...\n",
      "Tokenizing row 934001 of 6409801...\n",
      "Tokenizing row 935001 of 6409801...\n",
      "Tokenizing row 936001 of 6409801...\n",
      "Tokenizing row 937001 of 6409801...\n",
      "Tokenizing row 938001 of 6409801...\n",
      "Tokenizing row 939001 of 6409801...\n",
      "Tokenizing row 940001 of 6409801...\n",
      "Tokenizing row 941001 of 6409801...\n",
      "Tokenizing row 942001 of 6409801...\n",
      "Tokenizing row 943001 of 6409801...\n",
      "Tokenizing row 944001 of 6409801...\n",
      "Tokenizing row 945001 of 6409801...\n",
      "Tokenizing row 946001 of 6409801...\n",
      "Tokenizing row 947001 of 6409801...\n",
      "Tokenizing row 948001 of 6409801...\n",
      "Tokenizing row 949001 of 6409801...\n",
      "Tokenizing row 950001 of 6409801...\n",
      "Tokenizing row 951001 of 6409801...\n",
      "Tokenizing row 952001 of 6409801...\n",
      "Tokenizing row 953001 of 6409801...\n",
      "Tokenizing row 954001 of 6409801...\n",
      "Tokenizing row 955001 of 6409801...\n",
      "Tokenizing row 956001 of 6409801...\n",
      "Tokenizing row 957001 of 6409801...\n",
      "Tokenizing row 958001 of 6409801...\n",
      "Tokenizing row 959001 of 6409801...\n",
      "Tokenizing row 960001 of 6409801...\n",
      "Tokenizing row 961001 of 6409801...\n",
      "Tokenizing row 962001 of 6409801...\n",
      "Tokenizing row 963001 of 6409801...\n",
      "Tokenizing row 964001 of 6409801...\n",
      "Tokenizing row 965001 of 6409801...\n",
      "Tokenizing row 966001 of 6409801...\n",
      "Tokenizing row 967001 of 6409801...\n",
      "Tokenizing row 968001 of 6409801...\n",
      "Tokenizing row 969001 of 6409801...\n",
      "Tokenizing row 970001 of 6409801...\n",
      "Tokenizing row 971001 of 6409801...\n",
      "Tokenizing row 972001 of 6409801...\n",
      "Tokenizing row 973001 of 6409801...\n",
      "Tokenizing row 974001 of 6409801...\n",
      "Tokenizing row 975001 of 6409801...\n",
      "Tokenizing row 976001 of 6409801...\n",
      "Tokenizing row 977001 of 6409801...\n",
      "Tokenizing row 978001 of 6409801...\n",
      "Tokenizing row 979001 of 6409801...\n",
      "Tokenizing row 980001 of 6409801...\n",
      "Tokenizing row 981001 of 6409801...\n",
      "Tokenizing row 982001 of 6409801...\n",
      "Tokenizing row 983001 of 6409801...\n",
      "Tokenizing row 984001 of 6409801...\n",
      "Tokenizing row 985001 of 6409801...\n",
      "Tokenizing row 986001 of 6409801...\n",
      "Tokenizing row 987001 of 6409801...\n",
      "Tokenizing row 988001 of 6409801...\n",
      "Tokenizing row 989001 of 6409801...\n",
      "Tokenizing row 990001 of 6409801...\n",
      "Tokenizing row 991001 of 6409801...\n",
      "Tokenizing row 992001 of 6409801...\n",
      "Tokenizing row 993001 of 6409801...\n",
      "Tokenizing row 994001 of 6409801...\n",
      "Tokenizing row 995001 of 6409801...\n",
      "Tokenizing row 996001 of 6409801...\n",
      "Tokenizing row 997001 of 6409801...\n",
      "Tokenizing row 998001 of 6409801...\n",
      "Tokenizing row 999001 of 6409801...\n",
      "Tokenizing row 1000001 of 6409801...\n",
      "Tokenizing row 1001001 of 6409801...\n",
      "Tokenizing row 1002001 of 6409801...\n",
      "Tokenizing row 1003001 of 6409801...\n",
      "Tokenizing row 1004001 of 6409801...\n",
      "Tokenizing row 1005001 of 6409801...\n",
      "Tokenizing row 1006001 of 6409801...\n",
      "Tokenizing row 1007001 of 6409801...\n",
      "Tokenizing row 1008001 of 6409801...\n",
      "Tokenizing row 1009001 of 6409801...\n",
      "Tokenizing row 1010001 of 6409801...\n",
      "Tokenizing row 1011001 of 6409801...\n",
      "Tokenizing row 1012001 of 6409801...\n",
      "Tokenizing row 1013001 of 6409801...\n",
      "Tokenizing row 1014001 of 6409801...\n",
      "Tokenizing row 1015001 of 6409801...\n",
      "Tokenizing row 1016001 of 6409801...\n",
      "Tokenizing row 1017001 of 6409801...\n",
      "Tokenizing row 1018001 of 6409801...\n",
      "Tokenizing row 1019001 of 6409801...\n",
      "Tokenizing row 1020001 of 6409801...\n",
      "Tokenizing row 1021001 of 6409801...\n",
      "Tokenizing row 1022001 of 6409801...\n",
      "Tokenizing row 1023001 of 6409801...\n",
      "Tokenizing row 1024001 of 6409801...\n",
      "Tokenizing row 1025001 of 6409801...\n",
      "Tokenizing row 1026001 of 6409801...\n",
      "Tokenizing row 1027001 of 6409801...\n",
      "Tokenizing row 1028001 of 6409801...\n",
      "Tokenizing row 1029001 of 6409801...\n",
      "Tokenizing row 1030001 of 6409801...\n",
      "Tokenizing row 1031001 of 6409801...\n",
      "Tokenizing row 1032001 of 6409801...\n",
      "Tokenizing row 1033001 of 6409801...\n",
      "Tokenizing row 1034001 of 6409801...\n",
      "Tokenizing row 1035001 of 6409801...\n",
      "Tokenizing row 1036001 of 6409801...\n",
      "Tokenizing row 1037001 of 6409801...\n",
      "Tokenizing row 1038001 of 6409801...\n",
      "Tokenizing row 1039001 of 6409801...\n",
      "Tokenizing row 1040001 of 6409801...\n",
      "Tokenizing row 1041001 of 6409801...\n",
      "Tokenizing row 1042001 of 6409801...\n",
      "Tokenizing row 1043001 of 6409801...\n",
      "Tokenizing row 1044001 of 6409801...\n",
      "Tokenizing row 1045001 of 6409801...\n",
      "Tokenizing row 1046001 of 6409801...\n",
      "Tokenizing row 1047001 of 6409801...\n",
      "Tokenizing row 1048001 of 6409801...\n",
      "Tokenizing row 1049001 of 6409801...\n",
      "Tokenizing row 1050001 of 6409801...\n",
      "Tokenizing row 1051001 of 6409801...\n",
      "Tokenizing row 1052001 of 6409801...\n",
      "Tokenizing row 1053001 of 6409801...\n",
      "Tokenizing row 1054001 of 6409801...\n",
      "Tokenizing row 1055001 of 6409801...\n",
      "Tokenizing row 1056001 of 6409801...\n",
      "Tokenizing row 1057001 of 6409801...\n",
      "Tokenizing row 1058001 of 6409801...\n",
      "Tokenizing row 1059001 of 6409801...\n",
      "Tokenizing row 1060001 of 6409801...\n",
      "Tokenizing row 1061001 of 6409801...\n",
      "Tokenizing row 1062001 of 6409801...\n",
      "Tokenizing row 1063001 of 6409801...\n",
      "Tokenizing row 1064001 of 6409801...\n",
      "Tokenizing row 1065001 of 6409801...\n",
      "Tokenizing row 1066001 of 6409801...\n",
      "Tokenizing row 1067001 of 6409801...\n",
      "Tokenizing row 1068001 of 6409801...\n",
      "Tokenizing row 1069001 of 6409801...\n",
      "Tokenizing row 1070001 of 6409801...\n",
      "Tokenizing row 1071001 of 6409801...\n",
      "Tokenizing row 1072001 of 6409801...\n",
      "Tokenizing row 1073001 of 6409801...\n",
      "Tokenizing row 1074001 of 6409801...\n",
      "Tokenizing row 1075001 of 6409801...\n",
      "Tokenizing row 1076001 of 6409801...\n",
      "Tokenizing row 1077001 of 6409801...\n",
      "Tokenizing row 1078001 of 6409801...\n",
      "Tokenizing row 1079001 of 6409801...\n",
      "Tokenizing row 1080001 of 6409801...\n",
      "Tokenizing row 1081001 of 6409801...\n",
      "Tokenizing row 1082001 of 6409801...\n",
      "Tokenizing row 1083001 of 6409801...\n",
      "Tokenizing row 1084001 of 6409801...\n",
      "Tokenizing row 1085001 of 6409801...\n",
      "Tokenizing row 1086001 of 6409801...\n",
      "Tokenizing row 1087001 of 6409801...\n",
      "Tokenizing row 1088001 of 6409801...\n",
      "Tokenizing row 1089001 of 6409801...\n",
      "Tokenizing row 1090001 of 6409801...\n",
      "Tokenizing row 1091001 of 6409801...\n",
      "Tokenizing row 1092001 of 6409801...\n",
      "Tokenizing row 1093001 of 6409801...\n",
      "Tokenizing row 1094001 of 6409801...\n",
      "Tokenizing row 1095001 of 6409801...\n",
      "Tokenizing row 1096001 of 6409801...\n",
      "Tokenizing row 1097001 of 6409801...\n",
      "Tokenizing row 1098001 of 6409801...\n",
      "Tokenizing row 1099001 of 6409801...\n",
      "Tokenizing row 1100001 of 6409801...\n",
      "Tokenizing row 1101001 of 6409801...\n",
      "Tokenizing row 1102001 of 6409801...\n",
      "Tokenizing row 1103001 of 6409801...\n",
      "Tokenizing row 1104001 of 6409801...\n",
      "Tokenizing row 1105001 of 6409801...\n",
      "Tokenizing row 1106001 of 6409801...\n",
      "Tokenizing row 1107001 of 6409801...\n",
      "Tokenizing row 1108001 of 6409801...\n",
      "Tokenizing row 1109001 of 6409801...\n",
      "Tokenizing row 1110001 of 6409801...\n",
      "Tokenizing row 1111001 of 6409801...\n",
      "Tokenizing row 1112001 of 6409801...\n",
      "Tokenizing row 1113001 of 6409801...\n",
      "Tokenizing row 1114001 of 6409801...\n",
      "Tokenizing row 1115001 of 6409801...\n",
      "Tokenizing row 1116001 of 6409801...\n",
      "Tokenizing row 1117001 of 6409801...\n",
      "Tokenizing row 1118001 of 6409801...\n",
      "Tokenizing row 1119001 of 6409801...\n",
      "Tokenizing row 1120001 of 6409801...\n",
      "Tokenizing row 1121001 of 6409801...\n",
      "Tokenizing row 1122001 of 6409801...\n",
      "Tokenizing row 1123001 of 6409801...\n",
      "Tokenizing row 1124001 of 6409801...\n",
      "Tokenizing row 1125001 of 6409801...\n",
      "Tokenizing row 1126001 of 6409801...\n",
      "Tokenizing row 1127001 of 6409801...\n",
      "Tokenizing row 1128001 of 6409801...\n",
      "Tokenizing row 1129001 of 6409801...\n",
      "Tokenizing row 1130001 of 6409801...\n",
      "Tokenizing row 1131001 of 6409801...\n",
      "Tokenizing row 1132001 of 6409801...\n",
      "Tokenizing row 1133001 of 6409801...\n",
      "Tokenizing row 1134001 of 6409801...\n",
      "Tokenizing row 1135001 of 6409801...\n",
      "Tokenizing row 1136001 of 6409801...\n",
      "Tokenizing row 1137001 of 6409801...\n",
      "Tokenizing row 1138001 of 6409801...\n",
      "Tokenizing row 1139001 of 6409801...\n",
      "Tokenizing row 1140001 of 6409801...\n",
      "Tokenizing row 1141001 of 6409801...\n",
      "Tokenizing row 1142001 of 6409801...\n",
      "Tokenizing row 1143001 of 6409801...\n",
      "Tokenizing row 1144001 of 6409801...\n",
      "Tokenizing row 1145001 of 6409801...\n",
      "Tokenizing row 1146001 of 6409801...\n",
      "Tokenizing row 1147001 of 6409801...\n",
      "Tokenizing row 1148001 of 6409801...\n",
      "Tokenizing row 1149001 of 6409801...\n",
      "Tokenizing row 1150001 of 6409801...\n",
      "Tokenizing row 1151001 of 6409801...\n",
      "Tokenizing row 1152001 of 6409801...\n",
      "Tokenizing row 1153001 of 6409801...\n",
      "Tokenizing row 1154001 of 6409801...\n",
      "Tokenizing row 1155001 of 6409801...\n",
      "Tokenizing row 1156001 of 6409801...\n",
      "Tokenizing row 1157001 of 6409801...\n",
      "Tokenizing row 1158001 of 6409801...\n",
      "Tokenizing row 1159001 of 6409801...\n",
      "Tokenizing row 1160001 of 6409801...\n",
      "Tokenizing row 1161001 of 6409801...\n",
      "Tokenizing row 1162001 of 6409801...\n",
      "Tokenizing row 1163001 of 6409801...\n",
      "Tokenizing row 1164001 of 6409801...\n",
      "Tokenizing row 1165001 of 6409801...\n",
      "Tokenizing row 1166001 of 6409801...\n",
      "Tokenizing row 1167001 of 6409801...\n",
      "Tokenizing row 1168001 of 6409801...\n",
      "Tokenizing row 1169001 of 6409801...\n",
      "Tokenizing row 1170001 of 6409801...\n",
      "Tokenizing row 1171001 of 6409801...\n",
      "Tokenizing row 1172001 of 6409801...\n",
      "Tokenizing row 1173001 of 6409801...\n",
      "Tokenizing row 1174001 of 6409801...\n",
      "Tokenizing row 1175001 of 6409801...\n",
      "Tokenizing row 1176001 of 6409801...\n",
      "Tokenizing row 1177001 of 6409801...\n",
      "Tokenizing row 1178001 of 6409801...\n",
      "Tokenizing row 1179001 of 6409801...\n",
      "Tokenizing row 1180001 of 6409801...\n",
      "Tokenizing row 1181001 of 6409801...\n",
      "Tokenizing row 1182001 of 6409801...\n",
      "Tokenizing row 1183001 of 6409801...\n",
      "Tokenizing row 1184001 of 6409801...\n",
      "Tokenizing row 1185001 of 6409801...\n",
      "Tokenizing row 1186001 of 6409801...\n",
      "Tokenizing row 1187001 of 6409801...\n",
      "Tokenizing row 1188001 of 6409801...\n",
      "Tokenizing row 1189001 of 6409801...\n",
      "Tokenizing row 1190001 of 6409801...\n",
      "Tokenizing row 1191001 of 6409801...\n",
      "Tokenizing row 1192001 of 6409801...\n",
      "Tokenizing row 1193001 of 6409801...\n",
      "Tokenizing row 1194001 of 6409801...\n",
      "Tokenizing row 1195001 of 6409801...\n",
      "Tokenizing row 1196001 of 6409801...\n",
      "Tokenizing row 1197001 of 6409801...\n",
      "Tokenizing row 1198001 of 6409801...\n",
      "Tokenizing row 1199001 of 6409801...\n",
      "Tokenizing row 1200001 of 6409801...\n",
      "Tokenizing row 1201001 of 6409801...\n",
      "Tokenizing row 1202001 of 6409801...\n",
      "Tokenizing row 1203001 of 6409801...\n",
      "Tokenizing row 1204001 of 6409801...\n",
      "Tokenizing row 1205001 of 6409801...\n",
      "Tokenizing row 1206001 of 6409801...\n",
      "Tokenizing row 1207001 of 6409801...\n",
      "Tokenizing row 1208001 of 6409801...\n",
      "Tokenizing row 1209001 of 6409801...\n",
      "Tokenizing row 1210001 of 6409801...\n",
      "Tokenizing row 1211001 of 6409801...\n",
      "Tokenizing row 1212001 of 6409801...\n",
      "Tokenizing row 1213001 of 6409801...\n",
      "Tokenizing row 1214001 of 6409801...\n",
      "Tokenizing row 1215001 of 6409801...\n",
      "Tokenizing row 1216001 of 6409801...\n",
      "Tokenizing row 1217001 of 6409801...\n",
      "Tokenizing row 1218001 of 6409801...\n",
      "Tokenizing row 1219001 of 6409801...\n",
      "Tokenizing row 1220001 of 6409801...\n",
      "Tokenizing row 1221001 of 6409801...\n",
      "Tokenizing row 1222001 of 6409801...\n",
      "Tokenizing row 1223001 of 6409801...\n",
      "Tokenizing row 1224001 of 6409801...\n",
      "Tokenizing row 1225001 of 6409801...\n",
      "Tokenizing row 1226001 of 6409801...\n",
      "Tokenizing row 1227001 of 6409801...\n",
      "Tokenizing row 1228001 of 6409801...\n",
      "Tokenizing row 1229001 of 6409801...\n",
      "Tokenizing row 1230001 of 6409801...\n",
      "Tokenizing row 1231001 of 6409801...\n",
      "Tokenizing row 1232001 of 6409801...\n",
      "Tokenizing row 1233001 of 6409801...\n",
      "Tokenizing row 1234001 of 6409801...\n",
      "Tokenizing row 1235001 of 6409801...\n",
      "Tokenizing row 1236001 of 6409801...\n",
      "Tokenizing row 1237001 of 6409801...\n",
      "Tokenizing row 1238001 of 6409801...\n",
      "Tokenizing row 1239001 of 6409801...\n",
      "Tokenizing row 1240001 of 6409801...\n",
      "Tokenizing row 1241001 of 6409801...\n",
      "Tokenizing row 1242001 of 6409801...\n",
      "Tokenizing row 1243001 of 6409801...\n",
      "Tokenizing row 1244001 of 6409801...\n",
      "Tokenizing row 1245001 of 6409801...\n",
      "Tokenizing row 1246001 of 6409801...\n",
      "Tokenizing row 1247001 of 6409801...\n",
      "Tokenizing row 1248001 of 6409801...\n",
      "Tokenizing row 1249001 of 6409801...\n",
      "Tokenizing row 1250001 of 6409801...\n",
      "Tokenizing row 1251001 of 6409801...\n",
      "Tokenizing row 1252001 of 6409801...\n",
      "Tokenizing row 1253001 of 6409801...\n",
      "Tokenizing row 1254001 of 6409801...\n",
      "Tokenizing row 1255001 of 6409801...\n",
      "Tokenizing row 1256001 of 6409801...\n",
      "Tokenizing row 1257001 of 6409801...\n",
      "Tokenizing row 1258001 of 6409801...\n",
      "Tokenizing row 1259001 of 6409801...\n",
      "Tokenizing row 1260001 of 6409801...\n",
      "Tokenizing row 1261001 of 6409801...\n",
      "Tokenizing row 1262001 of 6409801...\n",
      "Tokenizing row 1263001 of 6409801...\n",
      "Tokenizing row 1264001 of 6409801...\n",
      "Tokenizing row 1265001 of 6409801...\n",
      "Tokenizing row 1266001 of 6409801...\n",
      "Tokenizing row 1267001 of 6409801...\n",
      "Tokenizing row 1268001 of 6409801...\n",
      "Tokenizing row 1269001 of 6409801...\n",
      "Tokenizing row 1270001 of 6409801...\n",
      "Tokenizing row 1271001 of 6409801...\n",
      "Tokenizing row 1272001 of 6409801...\n",
      "Tokenizing row 1273001 of 6409801...\n",
      "Tokenizing row 1274001 of 6409801...\n",
      "Tokenizing row 1275001 of 6409801...\n",
      "Tokenizing row 1276001 of 6409801...\n",
      "Tokenizing row 1277001 of 6409801...\n",
      "Tokenizing row 1278001 of 6409801...\n",
      "Tokenizing row 1279001 of 6409801...\n",
      "Tokenizing row 1280001 of 6409801...\n",
      "Tokenizing row 1281001 of 6409801...\n",
      "Tokenizing row 1282001 of 6409801...\n",
      "Tokenizing row 1283001 of 6409801...\n",
      "Tokenizing row 1284001 of 6409801...\n",
      "Tokenizing row 1285001 of 6409801...\n",
      "Tokenizing row 1286001 of 6409801...\n",
      "Tokenizing row 1287001 of 6409801...\n",
      "Tokenizing row 1288001 of 6409801...\n",
      "Tokenizing row 1289001 of 6409801...\n",
      "Tokenizing row 1290001 of 6409801...\n",
      "Tokenizing row 1291001 of 6409801...\n",
      "Tokenizing row 1292001 of 6409801...\n",
      "Tokenizing row 1293001 of 6409801...\n",
      "Tokenizing row 1294001 of 6409801...\n",
      "Tokenizing row 1295001 of 6409801...\n",
      "Tokenizing row 1296001 of 6409801...\n",
      "Tokenizing row 1297001 of 6409801...\n",
      "Tokenizing row 1298001 of 6409801...\n",
      "Tokenizing row 1299001 of 6409801...\n",
      "Tokenizing row 1300001 of 6409801...\n",
      "Tokenizing row 1301001 of 6409801...\n",
      "Tokenizing row 1302001 of 6409801...\n",
      "Tokenizing row 1303001 of 6409801...\n",
      "Tokenizing row 1304001 of 6409801...\n",
      "Tokenizing row 1305001 of 6409801...\n",
      "Tokenizing row 1306001 of 6409801...\n",
      "Tokenizing row 1307001 of 6409801...\n",
      "Tokenizing row 1308001 of 6409801...\n",
      "Tokenizing row 1309001 of 6409801...\n",
      "Tokenizing row 1310001 of 6409801...\n",
      "Tokenizing row 1311001 of 6409801...\n",
      "Tokenizing row 1312001 of 6409801...\n",
      "Tokenizing row 1313001 of 6409801...\n",
      "Tokenizing row 1314001 of 6409801...\n",
      "Tokenizing row 1315001 of 6409801...\n",
      "Tokenizing row 1316001 of 6409801...\n",
      "Tokenizing row 1317001 of 6409801...\n",
      "Tokenizing row 1318001 of 6409801...\n",
      "Tokenizing row 1319001 of 6409801...\n",
      "Tokenizing row 1320001 of 6409801...\n",
      "Tokenizing row 1321001 of 6409801...\n",
      "Tokenizing row 1322001 of 6409801...\n",
      "Tokenizing row 1323001 of 6409801...\n",
      "Tokenizing row 1324001 of 6409801...\n",
      "Tokenizing row 1325001 of 6409801...\n",
      "Tokenizing row 1326001 of 6409801...\n",
      "Tokenizing row 1327001 of 6409801...\n",
      "Tokenizing row 1328001 of 6409801...\n",
      "Tokenizing row 1329001 of 6409801...\n",
      "Tokenizing row 1330001 of 6409801...\n",
      "Tokenizing row 1331001 of 6409801...\n",
      "Tokenizing row 1332001 of 6409801...\n",
      "Tokenizing row 1333001 of 6409801...\n",
      "Tokenizing row 1334001 of 6409801...\n",
      "Tokenizing row 1335001 of 6409801...\n",
      "Tokenizing row 1336001 of 6409801...\n",
      "Tokenizing row 1337001 of 6409801...\n",
      "Tokenizing row 1338001 of 6409801...\n",
      "Tokenizing row 1339001 of 6409801...\n",
      "Tokenizing row 1340001 of 6409801...\n",
      "Tokenizing row 1341001 of 6409801...\n",
      "Tokenizing row 1342001 of 6409801...\n",
      "Tokenizing row 1343001 of 6409801...\n",
      "Tokenizing row 1344001 of 6409801...\n",
      "Tokenizing row 1345001 of 6409801...\n",
      "Tokenizing row 1346001 of 6409801...\n",
      "Tokenizing row 1347001 of 6409801...\n",
      "Tokenizing row 1348001 of 6409801...\n",
      "Tokenizing row 1349001 of 6409801...\n",
      "Tokenizing row 1350001 of 6409801...\n",
      "Tokenizing row 1351001 of 6409801...\n",
      "Tokenizing row 1352001 of 6409801...\n",
      "Tokenizing row 1353001 of 6409801...\n",
      "Tokenizing row 1354001 of 6409801...\n",
      "Tokenizing row 1355001 of 6409801...\n",
      "Tokenizing row 1356001 of 6409801...\n",
      "Tokenizing row 1357001 of 6409801...\n",
      "Tokenizing row 1358001 of 6409801...\n",
      "Tokenizing row 1359001 of 6409801...\n",
      "Tokenizing row 1360001 of 6409801...\n",
      "Tokenizing row 1361001 of 6409801...\n",
      "Tokenizing row 1362001 of 6409801...\n",
      "Tokenizing row 1363001 of 6409801...\n",
      "Tokenizing row 1364001 of 6409801...\n",
      "Tokenizing row 1365001 of 6409801...\n",
      "Tokenizing row 1366001 of 6409801...\n",
      "Tokenizing row 1367001 of 6409801...\n",
      "Tokenizing row 1368001 of 6409801...\n",
      "Tokenizing row 1369001 of 6409801...\n",
      "Tokenizing row 1370001 of 6409801...\n",
      "Tokenizing row 1371001 of 6409801...\n",
      "Tokenizing row 1372001 of 6409801...\n",
      "Tokenizing row 1373001 of 6409801...\n",
      "Tokenizing row 1374001 of 6409801...\n",
      "Tokenizing row 1375001 of 6409801...\n",
      "Tokenizing row 1376001 of 6409801...\n",
      "Tokenizing row 1377001 of 6409801...\n",
      "Tokenizing row 1378001 of 6409801...\n",
      "Tokenizing row 1379001 of 6409801...\n",
      "Tokenizing row 1380001 of 6409801...\n",
      "Tokenizing row 1381001 of 6409801...\n",
      "Tokenizing row 1382001 of 6409801...\n",
      "Tokenizing row 1383001 of 6409801...\n",
      "Tokenizing row 1384001 of 6409801...\n",
      "Tokenizing row 1385001 of 6409801...\n",
      "Tokenizing row 1386001 of 6409801...\n",
      "Tokenizing row 1387001 of 6409801...\n",
      "Tokenizing row 1388001 of 6409801...\n",
      "Tokenizing row 1389001 of 6409801...\n",
      "Tokenizing row 1390001 of 6409801...\n",
      "Tokenizing row 1391001 of 6409801...\n",
      "Tokenizing row 1392001 of 6409801...\n",
      "Tokenizing row 1393001 of 6409801...\n",
      "Tokenizing row 1394001 of 6409801...\n",
      "Tokenizing row 1395001 of 6409801...\n",
      "Tokenizing row 1396001 of 6409801...\n",
      "Tokenizing row 1397001 of 6409801...\n",
      "Tokenizing row 1398001 of 6409801...\n",
      "Tokenizing row 1399001 of 6409801...\n",
      "Tokenizing row 1400001 of 6409801...\n",
      "Tokenizing row 1401001 of 6409801...\n",
      "Tokenizing row 1402001 of 6409801...\n",
      "Tokenizing row 1403001 of 6409801...\n",
      "Tokenizing row 1404001 of 6409801...\n",
      "Tokenizing row 1405001 of 6409801...\n",
      "Tokenizing row 1406001 of 6409801...\n",
      "Tokenizing row 1407001 of 6409801...\n",
      "Tokenizing row 1408001 of 6409801...\n",
      "Tokenizing row 1409001 of 6409801...\n",
      "Tokenizing row 1410001 of 6409801...\n",
      "Tokenizing row 1411001 of 6409801...\n",
      "Tokenizing row 1412001 of 6409801...\n",
      "Tokenizing row 1413001 of 6409801...\n",
      "Tokenizing row 1414001 of 6409801...\n",
      "Tokenizing row 1415001 of 6409801...\n",
      "Tokenizing row 1416001 of 6409801...\n",
      "Tokenizing row 1417001 of 6409801...\n",
      "Tokenizing row 1418001 of 6409801...\n",
      "Tokenizing row 1419001 of 6409801...\n",
      "Tokenizing row 1420001 of 6409801...\n",
      "Tokenizing row 1421001 of 6409801...\n",
      "Tokenizing row 1422001 of 6409801...\n",
      "Tokenizing row 1423001 of 6409801...\n",
      "Tokenizing row 1424001 of 6409801...\n",
      "Tokenizing row 1425001 of 6409801...\n",
      "Tokenizing row 1426001 of 6409801...\n",
      "Tokenizing row 1427001 of 6409801...\n",
      "Tokenizing row 1428001 of 6409801...\n",
      "Tokenizing row 1429001 of 6409801...\n",
      "Tokenizing row 1430001 of 6409801...\n",
      "Tokenizing row 1431001 of 6409801...\n",
      "Tokenizing row 1432001 of 6409801...\n",
      "Tokenizing row 1433001 of 6409801...\n",
      "Tokenizing row 1434001 of 6409801...\n",
      "Tokenizing row 1435001 of 6409801...\n",
      "Tokenizing row 1436001 of 6409801...\n",
      "Tokenizing row 1437001 of 6409801...\n",
      "Tokenizing row 1438001 of 6409801...\n",
      "Tokenizing row 1439001 of 6409801...\n",
      "Tokenizing row 1440001 of 6409801...\n",
      "Tokenizing row 1441001 of 6409801...\n",
      "Tokenizing row 1442001 of 6409801...\n",
      "Tokenizing row 1443001 of 6409801...\n",
      "Tokenizing row 1444001 of 6409801...\n",
      "Tokenizing row 1445001 of 6409801...\n",
      "Tokenizing row 1446001 of 6409801...\n",
      "Tokenizing row 1447001 of 6409801...\n",
      "Tokenizing row 1448001 of 6409801...\n",
      "Tokenizing row 1449001 of 6409801...\n",
      "Tokenizing row 1450001 of 6409801...\n",
      "Tokenizing row 1451001 of 6409801...\n",
      "Tokenizing row 1452001 of 6409801...\n",
      "Tokenizing row 1453001 of 6409801...\n",
      "Tokenizing row 1454001 of 6409801...\n",
      "Tokenizing row 1455001 of 6409801...\n",
      "Tokenizing row 1456001 of 6409801...\n",
      "Tokenizing row 1457001 of 6409801...\n",
      "Tokenizing row 1458001 of 6409801...\n",
      "Tokenizing row 1459001 of 6409801...\n",
      "Tokenizing row 1460001 of 6409801...\n",
      "Tokenizing row 1461001 of 6409801...\n",
      "Tokenizing row 1462001 of 6409801...\n",
      "Tokenizing row 1463001 of 6409801...\n",
      "Tokenizing row 1464001 of 6409801...\n",
      "Tokenizing row 1465001 of 6409801...\n",
      "Tokenizing row 1466001 of 6409801...\n",
      "Tokenizing row 1467001 of 6409801...\n",
      "Tokenizing row 1468001 of 6409801...\n",
      "Tokenizing row 1469001 of 6409801...\n",
      "Tokenizing row 1470001 of 6409801...\n",
      "Tokenizing row 1471001 of 6409801...\n",
      "Tokenizing row 1472001 of 6409801...\n",
      "Tokenizing row 1473001 of 6409801...\n",
      "Tokenizing row 1474001 of 6409801...\n",
      "Tokenizing row 1475001 of 6409801...\n",
      "Tokenizing row 1476001 of 6409801...\n",
      "Tokenizing row 1477001 of 6409801...\n",
      "Tokenizing row 1478001 of 6409801...\n",
      "Tokenizing row 1479001 of 6409801...\n",
      "Tokenizing row 1480001 of 6409801...\n",
      "Tokenizing row 1481001 of 6409801...\n",
      "Tokenizing row 1482001 of 6409801...\n",
      "Tokenizing row 1483001 of 6409801...\n",
      "Tokenizing row 1484001 of 6409801...\n",
      "Tokenizing row 1485001 of 6409801...\n",
      "Tokenizing row 1486001 of 6409801...\n",
      "Tokenizing row 1487001 of 6409801...\n",
      "Tokenizing row 1488001 of 6409801...\n",
      "Tokenizing row 1489001 of 6409801...\n",
      "Tokenizing row 1490001 of 6409801...\n",
      "Tokenizing row 1491001 of 6409801...\n",
      "Tokenizing row 1492001 of 6409801...\n",
      "Tokenizing row 1493001 of 6409801...\n",
      "Tokenizing row 1494001 of 6409801...\n",
      "Tokenizing row 1495001 of 6409801...\n",
      "Tokenizing row 1496001 of 6409801...\n",
      "Tokenizing row 1497001 of 6409801...\n",
      "Tokenizing row 1498001 of 6409801...\n",
      "Tokenizing row 1499001 of 6409801...\n",
      "Tokenizing row 1500001 of 6409801...\n",
      "Tokenizing row 1501001 of 6409801...\n",
      "Tokenizing row 1502001 of 6409801...\n",
      "Tokenizing row 1503001 of 6409801...\n",
      "Tokenizing row 1504001 of 6409801...\n",
      "Tokenizing row 1505001 of 6409801...\n",
      "Tokenizing row 1506001 of 6409801...\n",
      "Tokenizing row 1507001 of 6409801...\n",
      "Tokenizing row 1508001 of 6409801...\n",
      "Tokenizing row 1509001 of 6409801...\n",
      "Tokenizing row 1510001 of 6409801...\n",
      "Tokenizing row 1511001 of 6409801...\n",
      "Tokenizing row 1512001 of 6409801...\n",
      "Tokenizing row 1513001 of 6409801...\n",
      "Tokenizing row 1514001 of 6409801...\n",
      "Tokenizing row 1515001 of 6409801...\n",
      "Tokenizing row 1516001 of 6409801...\n",
      "Tokenizing row 1517001 of 6409801...\n",
      "Tokenizing row 1518001 of 6409801...\n",
      "Tokenizing row 1519001 of 6409801...\n",
      "Tokenizing row 1520001 of 6409801...\n",
      "Tokenizing row 1521001 of 6409801...\n",
      "Tokenizing row 1522001 of 6409801...\n",
      "Tokenizing row 1523001 of 6409801...\n",
      "Tokenizing row 1524001 of 6409801...\n",
      "Tokenizing row 1525001 of 6409801...\n",
      "Tokenizing row 1526001 of 6409801...\n",
      "Tokenizing row 1527001 of 6409801...\n",
      "Tokenizing row 1528001 of 6409801...\n",
      "Tokenizing row 1529001 of 6409801...\n",
      "Tokenizing row 1530001 of 6409801...\n",
      "Tokenizing row 1531001 of 6409801...\n",
      "Tokenizing row 1532001 of 6409801...\n",
      "Tokenizing row 1533001 of 6409801...\n",
      "Tokenizing row 1534001 of 6409801...\n",
      "Tokenizing row 1535001 of 6409801...\n",
      "Tokenizing row 1536001 of 6409801...\n",
      "Tokenizing row 1537001 of 6409801...\n",
      "Tokenizing row 1538001 of 6409801...\n",
      "Tokenizing row 1539001 of 6409801...\n",
      "Tokenizing row 1540001 of 6409801...\n",
      "Tokenizing row 1541001 of 6409801...\n",
      "Tokenizing row 1542001 of 6409801...\n",
      "Tokenizing row 1543001 of 6409801...\n",
      "Tokenizing row 1544001 of 6409801...\n",
      "Tokenizing row 1545001 of 6409801...\n",
      "Tokenizing row 1546001 of 6409801...\n",
      "Tokenizing row 1547001 of 6409801...\n",
      "Tokenizing row 1548001 of 6409801...\n",
      "Tokenizing row 1549001 of 6409801...\n",
      "Tokenizing row 1550001 of 6409801...\n",
      "Tokenizing row 1551001 of 6409801...\n",
      "Tokenizing row 1552001 of 6409801...\n",
      "Tokenizing row 1553001 of 6409801...\n",
      "Tokenizing row 1554001 of 6409801...\n",
      "Tokenizing row 1555001 of 6409801...\n",
      "Tokenizing row 1556001 of 6409801...\n",
      "Tokenizing row 1557001 of 6409801...\n",
      "Tokenizing row 1558001 of 6409801...\n",
      "Tokenizing row 1559001 of 6409801...\n",
      "Tokenizing row 1560001 of 6409801...\n",
      "Tokenizing row 1561001 of 6409801...\n",
      "Tokenizing row 1562001 of 6409801...\n",
      "Tokenizing row 1563001 of 6409801...\n",
      "Tokenizing row 1564001 of 6409801...\n",
      "Tokenizing row 1565001 of 6409801...\n",
      "Tokenizing row 1566001 of 6409801...\n",
      "Tokenizing row 1567001 of 6409801...\n",
      "Tokenizing row 1568001 of 6409801...\n",
      "Tokenizing row 1569001 of 6409801...\n",
      "Tokenizing row 1570001 of 6409801...\n",
      "Tokenizing row 1571001 of 6409801...\n",
      "Tokenizing row 1572001 of 6409801...\n",
      "Tokenizing row 1573001 of 6409801...\n",
      "Tokenizing row 1574001 of 6409801...\n",
      "Tokenizing row 1575001 of 6409801...\n",
      "Tokenizing row 1576001 of 6409801...\n",
      "Tokenizing row 1577001 of 6409801...\n",
      "Tokenizing row 1578001 of 6409801...\n",
      "Tokenizing row 1579001 of 6409801...\n",
      "Tokenizing row 1580001 of 6409801...\n",
      "Tokenizing row 1581001 of 6409801...\n",
      "Tokenizing row 1582001 of 6409801...\n",
      "Tokenizing row 1583001 of 6409801...\n",
      "Tokenizing row 1584001 of 6409801...\n",
      "Tokenizing row 1585001 of 6409801...\n",
      "Tokenizing row 1586001 of 6409801...\n",
      "Tokenizing row 1587001 of 6409801...\n",
      "Tokenizing row 1588001 of 6409801...\n",
      "Tokenizing row 1589001 of 6409801...\n",
      "Tokenizing row 1590001 of 6409801...\n",
      "Tokenizing row 1591001 of 6409801...\n",
      "Tokenizing row 1592001 of 6409801...\n",
      "Tokenizing row 1593001 of 6409801...\n",
      "Tokenizing row 1594001 of 6409801...\n",
      "Tokenizing row 1595001 of 6409801...\n",
      "Tokenizing row 1596001 of 6409801...\n",
      "Tokenizing row 1597001 of 6409801...\n",
      "Tokenizing row 1598001 of 6409801...\n",
      "Tokenizing row 1599001 of 6409801...\n",
      "Tokenizing row 1600001 of 6409801...\n",
      "Tokenizing row 1601001 of 6409801...\n",
      "Tokenizing row 1602001 of 6409801...\n",
      "Tokenizing row 1603001 of 6409801...\n",
      "Tokenizing row 1604001 of 6409801...\n",
      "Tokenizing row 1605001 of 6409801...\n",
      "Tokenizing row 1606001 of 6409801...\n",
      "Tokenizing row 1607001 of 6409801...\n",
      "Tokenizing row 1608001 of 6409801...\n",
      "Tokenizing row 1609001 of 6409801...\n",
      "Tokenizing row 1610001 of 6409801...\n",
      "Tokenizing row 1611001 of 6409801...\n",
      "Tokenizing row 1612001 of 6409801...\n",
      "Tokenizing row 1613001 of 6409801...\n",
      "Tokenizing row 1614001 of 6409801...\n",
      "Tokenizing row 1615001 of 6409801...\n",
      "Tokenizing row 1616001 of 6409801...\n",
      "Tokenizing row 1617001 of 6409801...\n",
      "Tokenizing row 1618001 of 6409801...\n",
      "Tokenizing row 1619001 of 6409801...\n",
      "Tokenizing row 1620001 of 6409801...\n",
      "Tokenizing row 1621001 of 6409801...\n",
      "Tokenizing row 1622001 of 6409801...\n",
      "Tokenizing row 1623001 of 6409801...\n",
      "Tokenizing row 1624001 of 6409801...\n",
      "Tokenizing row 1625001 of 6409801...\n",
      "Tokenizing row 1626001 of 6409801...\n",
      "Tokenizing row 1627001 of 6409801...\n",
      "Tokenizing row 1628001 of 6409801...\n",
      "Tokenizing row 1629001 of 6409801...\n",
      "Tokenizing row 1630001 of 6409801...\n",
      "Tokenizing row 1631001 of 6409801...\n",
      "Tokenizing row 1632001 of 6409801...\n",
      "Tokenizing row 1633001 of 6409801...\n",
      "Tokenizing row 1634001 of 6409801...\n",
      "Tokenizing row 1635001 of 6409801...\n",
      "Tokenizing row 1636001 of 6409801...\n",
      "Tokenizing row 1637001 of 6409801...\n",
      "Tokenizing row 1638001 of 6409801...\n",
      "Tokenizing row 1639001 of 6409801...\n",
      "Tokenizing row 1640001 of 6409801...\n",
      "Tokenizing row 1641001 of 6409801...\n",
      "Tokenizing row 1642001 of 6409801...\n",
      "Tokenizing row 1643001 of 6409801...\n",
      "Tokenizing row 1644001 of 6409801...\n",
      "Tokenizing row 1645001 of 6409801...\n",
      "Tokenizing row 1646001 of 6409801...\n",
      "Tokenizing row 1647001 of 6409801...\n",
      "Tokenizing row 1648001 of 6409801...\n",
      "Tokenizing row 1649001 of 6409801...\n",
      "Tokenizing row 1650001 of 6409801...\n",
      "Tokenizing row 1651001 of 6409801...\n",
      "Tokenizing row 1652001 of 6409801...\n",
      "Tokenizing row 1653001 of 6409801...\n",
      "Tokenizing row 1654001 of 6409801...\n",
      "Tokenizing row 1655001 of 6409801...\n",
      "Tokenizing row 1656001 of 6409801...\n",
      "Tokenizing row 1657001 of 6409801...\n",
      "Tokenizing row 1658001 of 6409801...\n",
      "Tokenizing row 1659001 of 6409801...\n",
      "Tokenizing row 1660001 of 6409801...\n",
      "Tokenizing row 1661001 of 6409801...\n",
      "Tokenizing row 1662001 of 6409801...\n",
      "Tokenizing row 1663001 of 6409801...\n",
      "Tokenizing row 1664001 of 6409801...\n",
      "Tokenizing row 1665001 of 6409801...\n",
      "Tokenizing row 1666001 of 6409801...\n",
      "Tokenizing row 1667001 of 6409801...\n",
      "Tokenizing row 1668001 of 6409801...\n",
      "Tokenizing row 1669001 of 6409801...\n",
      "Tokenizing row 1670001 of 6409801...\n",
      "Tokenizing row 1671001 of 6409801...\n",
      "Tokenizing row 1672001 of 6409801...\n",
      "Tokenizing row 1673001 of 6409801...\n",
      "Tokenizing row 1674001 of 6409801...\n",
      "Tokenizing row 1675001 of 6409801...\n",
      "Tokenizing row 1676001 of 6409801...\n",
      "Tokenizing row 1677001 of 6409801...\n",
      "Tokenizing row 1678001 of 6409801...\n",
      "Tokenizing row 1679001 of 6409801...\n",
      "Tokenizing row 1680001 of 6409801...\n",
      "Tokenizing row 1681001 of 6409801...\n",
      "Tokenizing row 1682001 of 6409801...\n",
      "Tokenizing row 1683001 of 6409801...\n",
      "Tokenizing row 1684001 of 6409801...\n",
      "Tokenizing row 1685001 of 6409801...\n",
      "Tokenizing row 1686001 of 6409801...\n",
      "Tokenizing row 1687001 of 6409801...\n",
      "Tokenizing row 1688001 of 6409801...\n",
      "Tokenizing row 1689001 of 6409801...\n",
      "Tokenizing row 1690001 of 6409801...\n",
      "Tokenizing row 1691001 of 6409801...\n",
      "Tokenizing row 1692001 of 6409801...\n",
      "Tokenizing row 1693001 of 6409801...\n",
      "Tokenizing row 1694001 of 6409801...\n",
      "Tokenizing row 1695001 of 6409801...\n",
      "Tokenizing row 1696001 of 6409801...\n",
      "Tokenizing row 1697001 of 6409801...\n",
      "Tokenizing row 1698001 of 6409801...\n",
      "Tokenizing row 1699001 of 6409801...\n",
      "Tokenizing row 1700001 of 6409801...\n",
      "Tokenizing row 1701001 of 6409801...\n",
      "Tokenizing row 1702001 of 6409801...\n",
      "Tokenizing row 1703001 of 6409801...\n",
      "Tokenizing row 1704001 of 6409801...\n",
      "Tokenizing row 1705001 of 6409801...\n",
      "Tokenizing row 1706001 of 6409801...\n",
      "Tokenizing row 1707001 of 6409801...\n",
      "Tokenizing row 1708001 of 6409801...\n",
      "Tokenizing row 1709001 of 6409801...\n",
      "Tokenizing row 1710001 of 6409801...\n",
      "Tokenizing row 1711001 of 6409801...\n",
      "Tokenizing row 1712001 of 6409801...\n",
      "Tokenizing row 1713001 of 6409801...\n",
      "Tokenizing row 1714001 of 6409801...\n",
      "Tokenizing row 1715001 of 6409801...\n",
      "Tokenizing row 1716001 of 6409801...\n",
      "Tokenizing row 1717001 of 6409801...\n",
      "Tokenizing row 1718001 of 6409801...\n",
      "Tokenizing row 1719001 of 6409801...\n",
      "Tokenizing row 1720001 of 6409801...\n",
      "Tokenizing row 1721001 of 6409801...\n",
      "Tokenizing row 1722001 of 6409801...\n",
      "Tokenizing row 1723001 of 6409801...\n",
      "Tokenizing row 1724001 of 6409801...\n",
      "Tokenizing row 1725001 of 6409801...\n",
      "Tokenizing row 1726001 of 6409801...\n",
      "Tokenizing row 1727001 of 6409801...\n",
      "Tokenizing row 1728001 of 6409801...\n",
      "Tokenizing row 1729001 of 6409801...\n",
      "Tokenizing row 1730001 of 6409801...\n",
      "Tokenizing row 1731001 of 6409801...\n",
      "Tokenizing row 1732001 of 6409801...\n",
      "Tokenizing row 1733001 of 6409801...\n",
      "Tokenizing row 1734001 of 6409801...\n",
      "Tokenizing row 1735001 of 6409801...\n",
      "Tokenizing row 1736001 of 6409801...\n",
      "Tokenizing row 1737001 of 6409801...\n",
      "Tokenizing row 1738001 of 6409801...\n",
      "Tokenizing row 1739001 of 6409801...\n",
      "Tokenizing row 1740001 of 6409801...\n",
      "Tokenizing row 1741001 of 6409801...\n",
      "Tokenizing row 1742001 of 6409801...\n",
      "Tokenizing row 1743001 of 6409801...\n",
      "Tokenizing row 1744001 of 6409801...\n",
      "Tokenizing row 1745001 of 6409801...\n",
      "Tokenizing row 1746001 of 6409801...\n",
      "Tokenizing row 1747001 of 6409801...\n",
      "Tokenizing row 1748001 of 6409801...\n",
      "Tokenizing row 1749001 of 6409801...\n",
      "Tokenizing row 1750001 of 6409801...\n",
      "Tokenizing row 1751001 of 6409801...\n",
      "Tokenizing row 1752001 of 6409801...\n",
      "Tokenizing row 1753001 of 6409801...\n",
      "Tokenizing row 1754001 of 6409801...\n",
      "Tokenizing row 1755001 of 6409801...\n",
      "Tokenizing row 1756001 of 6409801...\n",
      "Tokenizing row 1757001 of 6409801...\n",
      "Tokenizing row 1758001 of 6409801...\n",
      "Tokenizing row 1759001 of 6409801...\n",
      "Tokenizing row 1760001 of 6409801...\n",
      "Tokenizing row 1761001 of 6409801...\n",
      "Tokenizing row 1762001 of 6409801...\n",
      "Tokenizing row 1763001 of 6409801...\n",
      "Tokenizing row 1764001 of 6409801...\n",
      "Tokenizing row 1765001 of 6409801...\n",
      "Tokenizing row 1766001 of 6409801...\n",
      "Tokenizing row 1767001 of 6409801...\n",
      "Tokenizing row 1768001 of 6409801...\n",
      "Tokenizing row 1769001 of 6409801...\n",
      "Tokenizing row 1770001 of 6409801...\n",
      "Tokenizing row 1771001 of 6409801...\n",
      "Tokenizing row 1772001 of 6409801...\n",
      "Tokenizing row 1773001 of 6409801...\n",
      "Tokenizing row 1774001 of 6409801...\n",
      "Tokenizing row 1775001 of 6409801...\n",
      "Tokenizing row 1776001 of 6409801...\n",
      "Tokenizing row 1777001 of 6409801...\n",
      "Tokenizing row 1778001 of 6409801...\n",
      "Tokenizing row 1779001 of 6409801...\n",
      "Tokenizing row 1780001 of 6409801...\n",
      "Tokenizing row 1781001 of 6409801...\n",
      "Tokenizing row 1782001 of 6409801...\n",
      "Tokenizing row 1783001 of 6409801...\n",
      "Tokenizing row 1784001 of 6409801...\n",
      "Tokenizing row 1785001 of 6409801...\n",
      "Tokenizing row 1786001 of 6409801...\n",
      "Tokenizing row 1787001 of 6409801...\n",
      "Tokenizing row 1788001 of 6409801...\n",
      "Tokenizing row 1789001 of 6409801...\n",
      "Tokenizing row 1790001 of 6409801...\n",
      "Tokenizing row 1791001 of 6409801...\n",
      "Tokenizing row 1792001 of 6409801...\n",
      "Tokenizing row 1793001 of 6409801...\n",
      "Tokenizing row 1794001 of 6409801...\n",
      "Tokenizing row 1795001 of 6409801...\n",
      "Tokenizing row 1796001 of 6409801...\n",
      "Tokenizing row 1797001 of 6409801...\n",
      "Tokenizing row 1798001 of 6409801...\n",
      "Tokenizing row 1799001 of 6409801...\n",
      "Tokenizing row 1800001 of 6409801...\n",
      "Tokenizing row 1801001 of 6409801...\n",
      "Tokenizing row 1802001 of 6409801...\n",
      "Tokenizing row 1803001 of 6409801...\n",
      "Tokenizing row 1804001 of 6409801...\n",
      "Tokenizing row 1805001 of 6409801...\n",
      "Tokenizing row 1806001 of 6409801...\n",
      "Tokenizing row 1807001 of 6409801...\n",
      "Tokenizing row 1808001 of 6409801...\n",
      "Tokenizing row 1809001 of 6409801...\n",
      "Tokenizing row 1810001 of 6409801...\n",
      "Tokenizing row 1811001 of 6409801...\n",
      "Tokenizing row 1812001 of 6409801...\n",
      "Tokenizing row 1813001 of 6409801...\n",
      "Tokenizing row 1814001 of 6409801...\n",
      "Tokenizing row 1815001 of 6409801...\n",
      "Tokenizing row 1816001 of 6409801...\n",
      "Tokenizing row 1817001 of 6409801...\n",
      "Tokenizing row 1818001 of 6409801...\n",
      "Tokenizing row 1819001 of 6409801...\n",
      "Tokenizing row 1820001 of 6409801...\n",
      "Tokenizing row 1821001 of 6409801...\n",
      "Tokenizing row 1822001 of 6409801...\n",
      "Tokenizing row 1823001 of 6409801...\n",
      "Tokenizing row 1824001 of 6409801...\n",
      "Tokenizing row 1825001 of 6409801...\n",
      "Tokenizing row 1826001 of 6409801...\n",
      "Tokenizing row 1827001 of 6409801...\n",
      "Tokenizing row 1828001 of 6409801...\n",
      "Tokenizing row 1829001 of 6409801...\n",
      "Tokenizing row 1830001 of 6409801...\n",
      "Tokenizing row 1831001 of 6409801...\n",
      "Tokenizing row 1832001 of 6409801...\n",
      "Tokenizing row 1833001 of 6409801...\n",
      "Tokenizing row 1834001 of 6409801...\n",
      "Tokenizing row 1835001 of 6409801...\n",
      "Tokenizing row 1836001 of 6409801...\n",
      "Tokenizing row 1837001 of 6409801...\n",
      "Tokenizing row 1838001 of 6409801...\n",
      "Tokenizing row 1839001 of 6409801...\n",
      "Tokenizing row 1840001 of 6409801...\n",
      "Tokenizing row 1841001 of 6409801...\n",
      "Tokenizing row 1842001 of 6409801...\n",
      "Tokenizing row 1843001 of 6409801...\n",
      "Tokenizing row 1844001 of 6409801...\n",
      "Tokenizing row 1845001 of 6409801...\n",
      "Tokenizing row 1846001 of 6409801...\n",
      "Tokenizing row 1847001 of 6409801...\n",
      "Tokenizing row 1848001 of 6409801...\n",
      "Tokenizing row 1849001 of 6409801...\n",
      "Tokenizing row 1850001 of 6409801...\n",
      "Tokenizing row 1851001 of 6409801...\n",
      "Tokenizing row 1852001 of 6409801...\n",
      "Tokenizing row 1853001 of 6409801...\n",
      "Tokenizing row 1854001 of 6409801...\n",
      "Tokenizing row 1855001 of 6409801...\n",
      "Tokenizing row 1856001 of 6409801...\n",
      "Tokenizing row 1857001 of 6409801...\n",
      "Tokenizing row 1858001 of 6409801...\n",
      "Tokenizing row 1859001 of 6409801...\n",
      "Tokenizing row 1860001 of 6409801...\n",
      "Tokenizing row 1861001 of 6409801...\n",
      "Tokenizing row 1862001 of 6409801...\n",
      "Tokenizing row 1863001 of 6409801...\n",
      "Tokenizing row 1864001 of 6409801...\n",
      "Tokenizing row 1865001 of 6409801...\n",
      "Tokenizing row 1866001 of 6409801...\n",
      "Tokenizing row 1867001 of 6409801...\n",
      "Tokenizing row 1868001 of 6409801...\n",
      "Tokenizing row 1869001 of 6409801...\n",
      "Tokenizing row 1870001 of 6409801...\n",
      "Tokenizing row 1871001 of 6409801...\n",
      "Tokenizing row 1872001 of 6409801...\n",
      "Tokenizing row 1873001 of 6409801...\n",
      "Tokenizing row 1874001 of 6409801...\n",
      "Tokenizing row 1875001 of 6409801...\n",
      "Tokenizing row 1876001 of 6409801...\n",
      "Tokenizing row 1877001 of 6409801...\n",
      "Tokenizing row 1878001 of 6409801...\n",
      "Tokenizing row 1879001 of 6409801...\n",
      "Tokenizing row 1880001 of 6409801...\n",
      "Tokenizing row 1881001 of 6409801...\n",
      "Tokenizing row 1882001 of 6409801...\n",
      "Tokenizing row 1883001 of 6409801...\n",
      "Tokenizing row 1884001 of 6409801...\n",
      "Tokenizing row 1885001 of 6409801...\n",
      "Tokenizing row 1886001 of 6409801...\n",
      "Tokenizing row 1887001 of 6409801...\n",
      "Tokenizing row 1888001 of 6409801...\n",
      "Tokenizing row 1889001 of 6409801...\n",
      "Tokenizing row 1890001 of 6409801...\n",
      "Tokenizing row 1891001 of 6409801...\n",
      "Tokenizing row 1892001 of 6409801...\n",
      "Tokenizing row 1893001 of 6409801...\n",
      "Tokenizing row 1894001 of 6409801...\n",
      "Tokenizing row 1895001 of 6409801...\n",
      "Tokenizing row 1896001 of 6409801...\n",
      "Tokenizing row 1897001 of 6409801...\n",
      "Tokenizing row 1898001 of 6409801...\n",
      "Tokenizing row 1899001 of 6409801...\n",
      "Tokenizing row 1900001 of 6409801...\n",
      "Tokenizing row 1901001 of 6409801...\n",
      "Tokenizing row 1902001 of 6409801...\n",
      "Tokenizing row 1903001 of 6409801...\n",
      "Tokenizing row 1904001 of 6409801...\n",
      "Tokenizing row 1905001 of 6409801...\n",
      "Tokenizing row 1906001 of 6409801...\n",
      "Tokenizing row 1907001 of 6409801...\n",
      "Tokenizing row 1908001 of 6409801...\n",
      "Tokenizing row 1909001 of 6409801...\n",
      "Tokenizing row 1910001 of 6409801...\n",
      "Tokenizing row 1911001 of 6409801...\n",
      "Tokenizing row 1912001 of 6409801...\n",
      "Tokenizing row 1913001 of 6409801...\n",
      "Tokenizing row 1914001 of 6409801...\n",
      "Tokenizing row 1915001 of 6409801...\n",
      "Tokenizing row 1916001 of 6409801...\n",
      "Tokenizing row 1917001 of 6409801...\n",
      "Tokenizing row 1918001 of 6409801...\n",
      "Tokenizing row 1919001 of 6409801...\n",
      "Tokenizing row 1920001 of 6409801...\n",
      "Tokenizing row 1921001 of 6409801...\n",
      "Tokenizing row 1922001 of 6409801...\n",
      "Tokenizing row 1923001 of 6409801...\n",
      "Tokenizing row 1924001 of 6409801...\n",
      "Tokenizing row 1925001 of 6409801...\n",
      "Tokenizing row 1926001 of 6409801...\n",
      "Tokenizing row 1927001 of 6409801...\n",
      "Tokenizing row 1928001 of 6409801...\n",
      "Tokenizing row 1929001 of 6409801...\n",
      "Tokenizing row 1930001 of 6409801...\n",
      "Tokenizing row 1931001 of 6409801...\n",
      "Tokenizing row 1932001 of 6409801...\n",
      "Tokenizing row 1933001 of 6409801...\n",
      "Tokenizing row 1934001 of 6409801...\n",
      "Tokenizing row 1935001 of 6409801...\n",
      "Tokenizing row 1936001 of 6409801...\n",
      "Tokenizing row 1937001 of 6409801...\n",
      "Tokenizing row 1938001 of 6409801...\n",
      "Tokenizing row 1939001 of 6409801...\n",
      "Tokenizing row 1940001 of 6409801...\n",
      "Tokenizing row 1941001 of 6409801...\n",
      "Tokenizing row 1942001 of 6409801...\n",
      "Tokenizing row 1943001 of 6409801...\n",
      "Tokenizing row 1944001 of 6409801...\n",
      "Tokenizing row 1945001 of 6409801...\n",
      "Tokenizing row 1946001 of 6409801...\n",
      "Tokenizing row 1947001 of 6409801...\n",
      "Tokenizing row 1948001 of 6409801...\n",
      "Tokenizing row 1949001 of 6409801...\n",
      "Tokenizing row 1950001 of 6409801...\n",
      "Tokenizing row 1951001 of 6409801...\n",
      "Tokenizing row 1952001 of 6409801...\n",
      "Tokenizing row 1953001 of 6409801...\n",
      "Tokenizing row 1954001 of 6409801...\n",
      "Tokenizing row 1955001 of 6409801...\n",
      "Tokenizing row 1956001 of 6409801...\n",
      "Tokenizing row 1957001 of 6409801...\n",
      "Tokenizing row 1958001 of 6409801...\n",
      "Tokenizing row 1959001 of 6409801...\n",
      "Tokenizing row 1960001 of 6409801...\n",
      "Tokenizing row 1961001 of 6409801...\n",
      "Tokenizing row 1962001 of 6409801...\n",
      "Tokenizing row 1963001 of 6409801...\n",
      "Tokenizing row 1964001 of 6409801...\n",
      "Tokenizing row 1965001 of 6409801...\n",
      "Tokenizing row 1966001 of 6409801...\n",
      "Tokenizing row 1967001 of 6409801...\n",
      "Tokenizing row 1968001 of 6409801...\n",
      "Tokenizing row 1969001 of 6409801...\n",
      "Tokenizing row 1970001 of 6409801...\n",
      "Tokenizing row 1971001 of 6409801...\n",
      "Tokenizing row 1972001 of 6409801...\n",
      "Tokenizing row 1973001 of 6409801...\n",
      "Tokenizing row 1974001 of 6409801...\n",
      "Tokenizing row 1975001 of 6409801...\n",
      "Tokenizing row 1976001 of 6409801...\n",
      "Tokenizing row 1977001 of 6409801...\n",
      "Tokenizing row 1978001 of 6409801...\n",
      "Tokenizing row 1979001 of 6409801...\n",
      "Tokenizing row 1980001 of 6409801...\n",
      "Tokenizing row 1981001 of 6409801...\n",
      "Tokenizing row 1982001 of 6409801...\n",
      "Tokenizing row 1983001 of 6409801...\n",
      "Tokenizing row 1984001 of 6409801...\n",
      "Tokenizing row 1985001 of 6409801...\n",
      "Tokenizing row 1986001 of 6409801...\n",
      "Tokenizing row 1987001 of 6409801...\n",
      "Tokenizing row 1988001 of 6409801...\n",
      "Tokenizing row 1989001 of 6409801...\n",
      "Tokenizing row 1990001 of 6409801...\n",
      "Tokenizing row 1991001 of 6409801...\n",
      "Tokenizing row 1992001 of 6409801...\n",
      "Tokenizing row 1993001 of 6409801...\n",
      "Tokenizing row 1994001 of 6409801...\n",
      "Tokenizing row 1995001 of 6409801...\n",
      "Tokenizing row 1996001 of 6409801...\n",
      "Tokenizing row 1997001 of 6409801...\n",
      "Tokenizing row 1998001 of 6409801...\n",
      "Tokenizing row 1999001 of 6409801...\n",
      "Tokenizing row 2000001 of 6409801...\n",
      "Tokenizing row 2001001 of 6409801...\n",
      "Tokenizing row 2002001 of 6409801...\n",
      "Tokenizing row 2003001 of 6409801...\n",
      "Tokenizing row 2004001 of 6409801...\n",
      "Tokenizing row 2005001 of 6409801...\n",
      "Tokenizing row 2006001 of 6409801...\n",
      "Tokenizing row 2007001 of 6409801...\n",
      "Tokenizing row 2008001 of 6409801...\n",
      "Tokenizing row 2009001 of 6409801...\n",
      "Tokenizing row 2010001 of 6409801...\n",
      "Tokenizing row 2011001 of 6409801...\n",
      "Tokenizing row 2012001 of 6409801...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Tokenization Function with Progress Logging\n",
    "def tokenize_reviews_nltk(df, review_column):\n",
    "    total_rows = len(df)\n",
    "    print(f\"Starting tokenization for {total_rows} rows with NLTK...\")\n",
    "    for idx in range(total_rows):\n",
    "        if idx % 1000 == 0 or idx == total_rows - 1:  # Progress log every 1000 rows or on the last row\n",
    "            print(f\"Tokenizing row {idx + 1} of {total_rows}...\")\n",
    "        df.loc[idx, review_column] = word_tokenize(str(df.loc[idx, review_column]).lower())\n",
    "    return df\n",
    "\n",
    "# Tokenize Cleaned Steam Data\n",
    "print(\"Loading cleaned Steam data...\")\n",
    "steam_data = pd.read_csv(\"Datasets/cleaned_steam.csv\")\n",
    "print(\"Tokenizing Steam data with NLTK...\")\n",
    "steam_data = tokenize_reviews_nltk(steam_data, 'review')\n",
    "print(\"Steam data tokenization complete!\")\n",
    "\n",
    "# Tokenize Cleaned Yelp Data\n",
    "print(\"Loading cleaned Yelp data...\")\n",
    "yelp_data = pd.read_csv(\"Datasets/cleaned_yelp.csv\")\n",
    "print(\"Tokenizing Yelp data with NLTK...\")\n",
    "yelp_data = tokenize_reviews_nltk(yelp_data, 'review')\n",
    "print(\"Yelp data tokenization complete!\")\n",
    "\n",
    "# Tokenize Cleaned IMDb Data\n",
    "print(\"Loading cleaned IMDb data...\")\n",
    "imdb_data = pd.read_csv(\"Datasets/cleaned_imdb.csv\")\n",
    "print(\"Tokenizing IMDb data with NLTK...\")\n",
    "imdb_data = tokenize_reviews_nltk(imdb_data, 'review')\n",
    "print(\"IMDb data tokenization complete!\")\n",
    "\n",
    "# Tokenize Cleaned Amazon Data\n",
    "print(\"Loading cleaned Amazon data...\")\n",
    "amazon_data = pd.read_csv(\"Datasets/cleaned_amazon.csv\")\n",
    "print(\"Tokenizing Amazon data with NLTK...\")\n",
    "amazon_data = tokenize_reviews_nltk(amazon_data, 'review')\n",
    "print(\"Amazon data tokenization complete!\")\n",
    "\n",
    "# Save Tokenized Data to CSV\n",
    "print(\"Saving tokenized data...\")\n",
    "steam_data.to_csv(\"Datasets/tokenized_steam.csv\", index=False)\n",
    "yelp_data.to_csv(\"Datasets/tokenized_yelp.csv\", index=False)\n",
    "imdb_data.to_csv(\"Datasets/tokenized_imdb.csv\", index=False)\n",
    "amazon_data.to_csv(\"Datasets/tokenized_amazon.csv\", index=False)\n",
    "print(\"All tokenized data saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkgXL5FPQ3bx"
   },
   "source": [
    "## TF-IDF Matrices and Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KegyO5U3Q288",
    "outputId": "cd95d847-e5d5-4381-a789-251f56051cf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenized Steam data...\n",
      "Loading tokenized Yelp data...\n",
      "Loading tokenized IMDb data...\n",
      "Loading tokenized Amazon data...\n",
      "Converting tokenized reviews to strings...\n",
      "Vectorizing Steam data...\n",
      "Applying TF-IDF vectorization (max_features=5000)...\n",
      "TF-IDF vectorization complete!\n",
      "Vectorizing Yelp data...\n",
      "Applying TF-IDF vectorization (max_features=5000)...\n",
      "TF-IDF vectorization complete!\n",
      "Vectorizing IMDb data...\n",
      "Applying TF-IDF vectorization (max_features=5000)...\n",
      "TF-IDF vectorization complete!\n",
      "Vectorizing Amazon data...\n",
      "Applying TF-IDF vectorization (max_features=5000)...\n",
      "TF-IDF vectorization complete!\n",
      "Saving TF-IDF matrices and vectorizers...\n",
      "TF-IDF data saved!\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "####################\n",
    "##TF-ID##\n",
    "################\n",
    "############\n",
    "import pandas as pd\n",
    "# Load tokenized datasets\n",
    "print(\"Loading tokenized Steam data...\")\n",
    "steam_data = pd.read_csv(\"Datasets/tokenized_steam.csv\")\n",
    "print(\"Loading tokenized Yelp data...\")\n",
    "yelp_data = pd.read_csv(\"Datasets/tokenized_yelp.csv\")\n",
    "print(\"Loading tokenized IMDb data...\")\n",
    "imdb_data = pd.read_csv(\"Datasets/tokenized_imdb.csv\")\n",
    "print(\"Loading tokenized Amazon data...\")\n",
    "amazon_data = pd.read_csv(\"Datasets/tokenized_amazon.csv\")\n",
    "\n",
    "print(\"Converting tokenized reviews to strings...\")\n",
    "# steam_data['review'] = steam_data['review'].apply(lambda tokens: ' '.join(eval(tokens)))\n",
    "yelp_data['review'] = yelp_data['review'].apply(lambda tokens: ' '.join(eval(tokens)))\n",
    "imdb_data['review'] = imdb_data['review'].apply(lambda tokens: ' '.join(eval(tokens)))\n",
    "amazon_data['review'] = amazon_data['review'].apply(lambda tokens: ' '.join(eval(tokens)))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def vectorize_reviews(data, review_column, max_features=5000):\n",
    "    print(f\"Applying TF-IDF vectorization (max_features={max_features})...\")\n",
    "    vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "    tfidf_matrix = vectorizer.fit_transform(data[review_column])\n",
    "    print(\"TF-IDF vectorization complete!\")\n",
    "    return tfidf_matrix, vectorizer\n",
    "\n",
    "# Vectorize each dataset\n",
    "print(\"Vectorizing Steam data...\")\n",
    "steam_tfidf, steam_vectorizer = vectorize_reviews(steam_data, 'review')\n",
    "\n",
    "print(\"Vectorizing Yelp data...\")\n",
    "yelp_tfidf, yelp_vectorizer = vectorize_reviews(yelp_data, 'review')\n",
    "\n",
    "print(\"Vectorizing IMDb data...\")\n",
    "imdb_tfidf, imdb_vectorizer = vectorize_reviews(imdb_data, 'review')\n",
    "\n",
    "print(\"Vectorizing Amazon data...\")\n",
    "amazon_tfidf, amazon_vectorizer = vectorize_reviews(amazon_data, 'review')\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Save TF-IDF matrices and vectorizers\n",
    "print(\"Saving TF-IDF matrices and vectorizers...\")\n",
    "joblib.dump(steam_tfidf, \"Datasets/steam_tfidf.pkl\")\n",
    "joblib.dump(steam_vectorizer, \"Datasets/steam_vectorizer.pkl\")\n",
    "joblib.dump(yelp_tfidf, \"Datasets/yelp_tfidf.pkl\")\n",
    "joblib.dump(yelp_vectorizer, \"Datasets/yelp_vectorizer.pkl\")\n",
    "joblib.dump(imdb_tfidf, \"Datasets/imdb_tfidf.pkl\")\n",
    "joblib.dump(imdb_vectorizer, \"Datasets/imdb_vectorizer.pkl\")\n",
    "joblib.dump(amazon_tfidf, \"Datasets/amazon_tfidf.pkl\")\n",
    "joblib.dump(amazon_vectorizer, \"Datasets/amazon_vectorizer.pkl\")\n",
    "print(\"TF-IDF data saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3mYvSWlQaAG"
   },
   "source": [
    "## Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "az0Cm8i0QY64",
    "outputId": "702e208e-e4c9-427f-f409-eb8964e77b1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TF-IDF matrix and labels for Steam data...\n",
      "Splitting Steam data into training and testing sets...\n",
      "Steam data split: 5127840 training rows, 1281961 testing rows.\n",
      "Loading TF-IDF matrix and labels for Yelp data...\n",
      "Splitting Yelp data into training and testing sets...\n",
      "Yelp data split: 562 training rows, 141 testing rows.\n",
      "Loading TF-IDF matrix and labels for IMDb data...\n",
      "Splitting IMDb data into training and testing sets...\n",
      "IMDb data split: 40000 training rows, 10000 testing rows.\n",
      "Loading TF-IDF matrix and labels for Amazon data...\n",
      "Splitting Amazon data into training and testing sets...\n",
      "Amazon data split: 454763 training rows, 113691 testing rows.\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "####################\n",
    "##DATA SPLIT##\n",
    "################\n",
    "############\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Load TF-IDF matrix and sentiment labels\n",
    "print(\"Loading TF-IDF matrix and labels for Steam data...\")\n",
    "steam_tfidf = joblib.load(\"Datasets/steam_tfidf.pkl\")\n",
    "steam_data = pd.read_csv(\"Datasets/tokenized_steam.csv\")\n",
    "steam_labels = steam_data['sentiment'].map({'positive': 1, 'negative': 0})  # Map labels to binary values\n",
    "\n",
    "# Split Steam Data\n",
    "print(\"Splitting Steam data into training and testing sets...\")\n",
    "X_train_steam, X_test_steam, y_train_steam, y_test_steam = train_test_split(\n",
    "    steam_tfidf, steam_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "print(f\"Steam data split: {X_train_steam.shape[0]} training rows, {X_test_steam.shape[0]} testing rows.\")\n",
    "\n",
    "# Repeat for Yelp Data\n",
    "print(\"Loading TF-IDF matrix and labels for Yelp data...\")\n",
    "yelp_tfidf = joblib.load(\"Datasets/yelp_tfidf.pkl\")\n",
    "yelp_data = pd.read_csv(\"Datasets/tokenized_yelp.csv\")\n",
    "yelp_labels = yelp_data['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "print(\"Splitting Yelp data into training and testing sets...\")\n",
    "X_train_yelp, X_test_yelp, y_train_yelp, y_test_yelp = train_test_split(\n",
    "    yelp_tfidf, yelp_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "print(f\"Yelp data split: {X_train_yelp.shape[0]} training rows, {X_test_yelp.shape[0]} testing rows.\")\n",
    "\n",
    "# Repeat for IMDb Data\n",
    "print(\"Loading TF-IDF matrix and labels for IMDb data...\")\n",
    "imdb_tfidf = joblib.load(\"Datasets/imdb_tfidf.pkl\")\n",
    "imdb_data = pd.read_csv(\"Datasets/tokenized_imdb.csv\")\n",
    "imdb_labels = imdb_data['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "print(\"Splitting IMDb data into training and testing sets...\")\n",
    "X_train_imdb, X_test_imdb, y_train_imdb, y_test_imdb = train_test_split(\n",
    "    imdb_tfidf, imdb_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "print(f\"IMDb data split: {X_train_imdb.shape[0]} training rows, {X_test_imdb.shape[0]} testing rows.\")\n",
    "\n",
    "# Repeat for Amazon Data\n",
    "print(\"Loading TF-IDF matrix and labels for Amazon data...\")\n",
    "amazon_tfidf = joblib.load(\"Datasets/amazon_tfidf.pkl\")\n",
    "amazon_data = pd.read_csv(\"Datasets/tokenized_amazon.csv\")\n",
    "amazon_labels = amazon_data['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "print(\"Splitting Amazon data into training and testing sets...\")\n",
    "X_train_amazon, X_test_amazon, y_train_amazon, y_test_amazon = train_test_split(\n",
    "    amazon_tfidf, amazon_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "print(f\"Amazon data split: {X_train_amazon.shape[0]} training rows, {X_test_amazon.shape[0]} testing rows.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdj-LhiOHOU_"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGCWRzRSQnin"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QKbK2KC6Gcgj",
    "outputId": "300ee0f8-9deb-4a8c-ec9b-a1b14de209b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Steam data...\n",
      "Training Logistic Regression model for Steam...\n",
      "Evaluating model...\n",
      "Steam Accuracy: 0.8846\n",
      "Classification Report for Steam:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.50      0.61    230912\n",
      "           1       0.90      0.97      0.93   1051049\n",
      "\n",
      "    accuracy                           0.88   1281961\n",
      "   macro avg       0.84      0.74      0.77   1281961\n",
      "weighted avg       0.88      0.88      0.87   1281961\n",
      "\n",
      "Confusion Matrix for Steam:\n",
      " [[ 115795  115117]\n",
      " [  32782 1018267]]\n",
      "Processing Yelp data...\n",
      "Training Logistic Regression model for Yelp...\n",
      "Evaluating model...\n",
      "Yelp Accuracy: 0.7943\n",
      "Classification Report for Yelp:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.77      0.79        71\n",
      "           1       0.78      0.81      0.80        70\n",
      "\n",
      "    accuracy                           0.79       141\n",
      "   macro avg       0.79      0.79      0.79       141\n",
      "weighted avg       0.79      0.79      0.79       141\n",
      "\n",
      "Confusion Matrix for Yelp:\n",
      " [[55 16]\n",
      " [13 57]]\n",
      "Processing IMDb data...\n",
      "Training Logistic Regression model for IMDb...\n",
      "Evaluating model...\n",
      "IMDb Accuracy: 0.8939\n",
      "Classification Report for IMDb:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89      4961\n",
      "           1       0.89      0.91      0.90      5039\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n",
      "Confusion Matrix for IMDb:\n",
      " [[4378  583]\n",
      " [ 478 4561]]\n",
      "Processing Amazon data...\n",
      "Training Logistic Regression model for Amazon...\n",
      "Evaluating model...\n",
      "Amazon Accuracy: 0.8971\n",
      "Classification Report for Amazon:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.69      0.74     24666\n",
      "           1       0.92      0.96      0.94     89025\n",
      "\n",
      "    accuracy                           0.90    113691\n",
      "   macro avg       0.86      0.82      0.84    113691\n",
      "weighted avg       0.89      0.90      0.89    113691\n",
      "\n",
      "Confusion Matrix for Amazon:\n",
      " [[16927  7739]\n",
      " [ 3958 85067]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Train and Evaluate a Model\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, dataset_name):\n",
    "    print(f\"Training Logistic Regression model for {dataset_name}...\")\n",
    "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Evaluating model...\")\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Model Performance Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{dataset_name} Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    print(f\"Classification Report for {dataset_name}:\\n\", classification_report(y_test, y_pred))\n",
    "    print(f\"Confusion Matrix for {dataset_name}:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train and Evaluate on Steam Data\n",
    "print(\"Processing Steam data...\")\n",
    "steam_model = train_and_evaluate_model(X_train_steam, X_test_steam, y_train_steam, y_test_steam, \"Steam\")\n",
    "\n",
    "# Train and Evaluate on Yelp Data\n",
    "print(\"Processing Yelp data...\")\n",
    "yelp_model = train_and_evaluate_model(X_train_yelp, X_test_yelp, y_train_yelp, y_test_yelp, \"Yelp\")\n",
    "\n",
    "# Train and Evaluate on IMDb Data\n",
    "print(\"Processing IMDb data...\")\n",
    "imdb_model = train_and_evaluate_model(X_train_imdb, X_test_imdb, y_train_imdb, y_test_imdb, \"IMDb\")\n",
    "\n",
    "# Train and Evaluate on Amazon Data\n",
    "print(\"Processing Amazon data...\")\n",
    "amazon_model = train_and_evaluate_model(X_train_amazon, X_test_amazon, y_train_amazon, y_test_amazon, \"Amazon\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lIXl3MCHZ9D"
   },
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wPnvOwu0GcWR",
    "outputId": "71bb4353-74b5-440d-9499-0c87abb356b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Steam data with Naive Bayes...\n",
      "Training Naive Bayes model for Steam...\n",
      "Evaluating model...\n",
      "Steam Naive Bayes Accuracy: 0.8496\n",
      "Classification Report for Steam:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.20      0.32    230912\n",
      "           1       0.85      0.99      0.92   1051049\n",
      "\n",
      "    accuracy                           0.85   1281961\n",
      "   macro avg       0.86      0.59      0.62   1281961\n",
      "weighted avg       0.85      0.85      0.81   1281961\n",
      "\n",
      "Confusion Matrix for Steam:\n",
      " [[  45403  185509]\n",
      " [   7284 1043765]]\n",
      "Processing Yelp data with Naive Bayes...\n",
      "Training Naive Bayes model for Yelp...\n",
      "Evaluating model...\n",
      "Yelp Naive Bayes Accuracy: 0.7518\n",
      "Classification Report for Yelp:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.69      0.74        71\n",
      "           1       0.72      0.81      0.77        70\n",
      "\n",
      "    accuracy                           0.75       141\n",
      "   macro avg       0.76      0.75      0.75       141\n",
      "weighted avg       0.76      0.75      0.75       141\n",
      "\n",
      "Confusion Matrix for Yelp:\n",
      " [[49 22]\n",
      " [13 57]]\n",
      "Processing IMDb data with Naive Bayes...\n",
      "Training Naive Bayes model for IMDb...\n",
      "Evaluating model...\n",
      "IMDb Naive Bayes Accuracy: 0.8528\n",
      "Classification Report for IMDb:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85      4961\n",
      "           1       0.86      0.85      0.85      5039\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.85      0.85      0.85     10000\n",
      "weighted avg       0.85      0.85      0.85     10000\n",
      "\n",
      "Confusion Matrix for IMDb:\n",
      " [[4240  721]\n",
      " [ 751 4288]]\n",
      "Processing Amazon data with Naive Bayes...\n",
      "Training Naive Bayes model for Amazon...\n",
      "Evaluating model...\n",
      "Amazon Naive Bayes Accuracy: 0.8356\n",
      "Classification Report for Amazon:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.27      0.42     24666\n",
      "           1       0.83      0.99      0.90     89025\n",
      "\n",
      "    accuracy                           0.84    113691\n",
      "   macro avg       0.86      0.63      0.66    113691\n",
      "weighted avg       0.85      0.84      0.80    113691\n",
      "\n",
      "Confusion Matrix for Amazon:\n",
      " [[ 6770 17896]\n",
      " [  792 88233]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Train and Evaluate a Naive Bayes Model\n",
    "def train_and_evaluate_naive_bayes(X_train, X_test, y_train, y_test, dataset_name):\n",
    "    print(f\"Training Naive Bayes model for {dataset_name}...\")\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Evaluating model...\")\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Model Performance Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{dataset_name} Naive Bayes Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    print(f\"Classification Report for {dataset_name}:\\n\", classification_report(y_test, y_pred))\n",
    "    print(f\"Confusion Matrix for {dataset_name}:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train and Evaluate on Steam Data\n",
    "print(\"Processing Steam data with Naive Bayes...\")\n",
    "steam_nb_model = train_and_evaluate_naive_bayes(X_train_steam, X_test_steam, y_train_steam, y_test_steam, \"Steam\")\n",
    "\n",
    "# Train and Evaluate on Yelp Data\n",
    "print(\"Processing Yelp data with Naive Bayes...\")\n",
    "yelp_nb_model = train_and_evaluate_naive_bayes(X_train_yelp, X_test_yelp, y_train_yelp, y_test_yelp, \"Yelp\")\n",
    "\n",
    "# Train and Evaluate on IMDb Data\n",
    "print(\"Processing IMDb data with Naive Bayes...\")\n",
    "imdb_nb_model = train_and_evaluate_naive_bayes(X_train_imdb, X_test_imdb, y_train_imdb, y_test_imdb, \"IMDb\")\n",
    "\n",
    "# Train and Evaluate on Amazon Data\n",
    "print(\"Processing Amazon data with Naive Bayes...\")\n",
    "amazon_nb_model = train_and_evaluate_naive_bayes(X_train_amazon, X_test_amazon, y_train_amazon, y_test_amazon, \"Amazon\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckoEL4-LQd0P"
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IXPMlddWGRSM",
    "outputId": "a1d67f3b-edaf-460a-face-d8c59587909e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Steam data with SVM...\n",
      "Training SVM model for Steam...\n",
      "Evaluating model...\n",
      "Steam SVM Accuracy: 0.8850\n",
      "Classification Report for Steam:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.49      0.61    230912\n",
      "           1       0.90      0.97      0.93   1051049\n",
      "\n",
      "    accuracy                           0.89   1281961\n",
      "   macro avg       0.84      0.73      0.77   1281961\n",
      "weighted avg       0.88      0.89      0.87   1281961\n",
      "\n",
      "Confusion Matrix for Steam:\n",
      " [[ 113570  117342]\n",
      " [  30039 1021010]]\n",
      "Processing Yelp data with SVM...\n",
      "Training SVM model for Yelp...\n",
      "Evaluating model...\n",
      "Yelp SVM Accuracy: 0.7943\n",
      "Classification Report for Yelp:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.75      0.79        71\n",
      "           1       0.77      0.84      0.80        70\n",
      "\n",
      "    accuracy                           0.79       141\n",
      "   macro avg       0.80      0.79      0.79       141\n",
      "weighted avg       0.80      0.79      0.79       141\n",
      "\n",
      "Confusion Matrix for Yelp:\n",
      " [[53 18]\n",
      " [11 59]]\n",
      "Processing IMDb data with SVM...\n",
      "Training SVM model for IMDb...\n",
      "Evaluating model...\n",
      "IMDb SVM Accuracy: 0.8890\n",
      "Classification Report for IMDb:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89      4961\n",
      "           1       0.88      0.90      0.89      5039\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n",
      "Confusion Matrix for IMDb:\n",
      " [[4360  601]\n",
      " [ 509 4530]]\n",
      "Processing Amazon data with SVM...\n",
      "Training SVM model for Amazon...\n",
      "Evaluating model...\n",
      "Amazon SVM Accuracy: 0.8988\n",
      "Classification Report for Amazon:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.70      0.75     24666\n",
      "           1       0.92      0.95      0.94     89025\n",
      "\n",
      "    accuracy                           0.90    113691\n",
      "   macro avg       0.86      0.83      0.84    113691\n",
      "weighted avg       0.90      0.90      0.90    113691\n",
      "\n",
      "Confusion Matrix for Amazon:\n",
      " [[17215  7451]\n",
      " [ 4051 84974]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Train and Evaluate an SVM Model\n",
    "def train_and_evaluate_svm(X_train, X_test, y_train, y_test, dataset_name):\n",
    "    print(f\"Training SVM model for {dataset_name}...\")\n",
    "    model = LinearSVC(random_state=42, max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Evaluating model...\")\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Model Performance Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{dataset_name} SVM Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    print(f\"Classification Report for {dataset_name}:\\n\", classification_report(y_test, y_pred))\n",
    "    print(f\"Confusion Matrix for {dataset_name}:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train and Evaluate on Steam Data\n",
    "print(\"Processing Steam data with SVM...\")\n",
    "steam_svm_model = train_and_evaluate_svm(X_train_steam, X_test_steam, y_train_steam, y_test_steam, \"Steam\")\n",
    "\n",
    "# Train and Evaluate on Yelp Data\n",
    "print(\"Processing Yelp data with SVM...\")\n",
    "yelp_svm_model = train_and_evaluate_svm(X_train_yelp, X_test_yelp, y_train_yelp, y_test_yelp, \"Yelp\")\n",
    "\n",
    "# Train and Evaluate on IMDb Data\n",
    "print(\"Processing IMDb data with SVM...\")\n",
    "imdb_svm_model = train_and_evaluate_svm(X_train_imdb, X_test_imdb, y_train_imdb, y_test_imdb, \"IMDb\")\n",
    "\n",
    "# Train and Evaluate on Amazon Data\n",
    "print(\"Processing Amazon data with SVM...\")\n",
    "amazon_svm_model = train_and_evaluate_svm(X_train_amazon, X_test_amazon, y_train_amazon, y_test_amazon, \"Amazon\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C61Q_WorQgLV"
   },
   "source": [
    "# Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWAeYBI1GRnV",
    "outputId": "6d47a174-ad56-4b82-ac29-dc39fb6f0a4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Logistic Regression on Steam data...\n",
      "Evaluating Naive Bayes on Steam data...\n",
      "Evaluating SVM on Steam data...\n",
      "Evaluating Logistic Regression on Yelp data...\n",
      "Evaluating Naive Bayes on Yelp data...\n",
      "Evaluating SVM on Yelp data...\n",
      "Evaluating Logistic Regression on IMDb data...\n",
      "Evaluating Naive Bayes on IMDb data...\n",
      "Evaluating SVM on IMDb data...\n",
      "Evaluating Logistic Regression on Amazon data...\n",
      "Evaluating Naive Bayes on Amazon data...\n",
      "Evaluating SVM on Amazon data...\n",
      "\n",
      "Model Performance Summary:\n",
      "   Dataset                Model  Accuracy  Precision (0)  Recall (0)  \\\n",
      "0    Steam  Logistic Regression  0.884631       0.779360    0.501468   \n",
      "1    Steam          Naive Bayes  0.849611       0.861750    0.196625   \n",
      "2    Steam                  SVM  0.885035       0.790828    0.491832   \n",
      "3     Yelp  Logistic Regression  0.794326       0.808824    0.774648   \n",
      "4     Yelp          Naive Bayes  0.751773       0.790323    0.690141   \n",
      "5     Yelp                  SVM  0.794326       0.828125    0.746479   \n",
      "6     IMDb  Logistic Regression  0.893900       0.901565    0.882483   \n",
      "7     IMDb          Naive Bayes  0.852800       0.849529    0.854666   \n",
      "8     IMDb                  SVM  0.889000       0.895461    0.878855   \n",
      "9   Amazon  Logistic Regression  0.897116       0.810486    0.686248   \n",
      "10  Amazon          Naive Bayes  0.835625       0.895266    0.274467   \n",
      "11  Amazon                  SVM  0.898831       0.809508    0.697924   \n",
      "\n",
      "    F1-Score (0)  Precision (1)  Recall (1)  F1-Score (1)  \n",
      "0       0.610268       0.898431    0.968810      0.932294  \n",
      "1       0.320192       0.849091    0.993070      0.915454  \n",
      "2       0.606481       0.896919    0.971420      0.932684  \n",
      "3       0.791367       0.780822    0.814286      0.797203  \n",
      "4       0.736842       0.721519    0.814286      0.765101  \n",
      "5       0.785185       0.766234    0.842857      0.802721  \n",
      "6       0.891922       0.886664    0.905140      0.895807  \n",
      "7       0.852090       0.856059    0.850962      0.853503  \n",
      "8       0.887080       0.882869    0.898988      0.890855  \n",
      "9       0.743211       0.916611    0.955541      0.935671  \n",
      "10      0.420132       0.831375    0.991104      0.904240  \n",
      "11      0.749586       0.919383    0.954496      0.936611  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Function to evaluate and store metrics\n",
    "def evaluate_model(model, X_test, y_test, dataset_name, model_name):\n",
    "    print(f\"Evaluating {model_name} on {dataset_name} data...\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)  # Get as dictionary\n",
    "\n",
    "    return {\n",
    "        \"Dataset\": dataset_name,\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision (0)\": report['0']['precision'],\n",
    "        \"Recall (0)\": report['0']['recall'],\n",
    "        \"F1-Score (0)\": report['0']['f1-score'],\n",
    "        \"Precision (1)\": report['1']['precision'],\n",
    "        \"Recall (1)\": report['1']['recall'],\n",
    "        \"F1-Score (1)\": report['1']['f1-score']\n",
    "    }\n",
    "\n",
    "# Collect results for all models and datasets\n",
    "results = []\n",
    "\n",
    "# Evaluate Steam Models\n",
    "results.append(evaluate_model(steam_model, X_test_steam, y_test_steam, \"Steam\", \"Logistic Regression\"))\n",
    "results.append(evaluate_model(steam_nb_model, X_test_steam, y_test_steam, \"Steam\", \"Naive Bayes\"))\n",
    "results.append(evaluate_model(steam_svm_model, X_test_steam, y_test_steam, \"Steam\", \"SVM\"))\n",
    "\n",
    "# Evaluate Yelp Models\n",
    "results.append(evaluate_model(yelp_model, X_test_yelp, y_test_yelp, \"Yelp\", \"Logistic Regression\"))\n",
    "results.append(evaluate_model(yelp_nb_model, X_test_yelp, y_test_yelp, \"Yelp\", \"Naive Bayes\"))\n",
    "results.append(evaluate_model(yelp_svm_model, X_test_yelp, y_test_yelp, \"Yelp\", \"SVM\"))\n",
    "\n",
    "# Evaluate IMDb Models\n",
    "results.append(evaluate_model(imdb_model, X_test_imdb, y_test_imdb, \"IMDb\", \"Logistic Regression\"))\n",
    "results.append(evaluate_model(imdb_nb_model, X_test_imdb, y_test_imdb, \"IMDb\", \"Naive Bayes\"))\n",
    "results.append(evaluate_model(imdb_svm_model, X_test_imdb, y_test_imdb, \"IMDb\", \"SVM\"))\n",
    "\n",
    "# Evaluate Amazon Models\n",
    "results.append(evaluate_model(amazon_model, X_test_amazon, y_test_amazon, \"Amazon\", \"Logistic Regression\"))\n",
    "results.append(evaluate_model(amazon_nb_model, X_test_amazon, y_test_amazon, \"Amazon\", \"Naive Bayes\"))\n",
    "results.append(evaluate_model(amazon_svm_model, X_test_amazon, y_test_amazon, \"Amazon\", \"SVM\"))\n",
    "\n",
    "# Create a summary table\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "print(results_df)\n",
    "\n",
    "# Save results to CSV for future reference\n",
    "results_df.to_csv(\"Model_Performance_Summary.csv\", index=False)\n",
    "\n",
    "#The F1 score is a performance metric that combines precision and recall into a single value.\n",
    "# $It is particularly useful when you want a balance between the two\n",
    "#1 Score ranges from 0 to 1:\n",
    "#1: Perfect balance between precision and recall.\n",
    "#0: No true positives at all.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTu5DqqMeguS"
   },
   "source": [
    "# Experimenting with Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqtWQbe8e05D"
   },
   "source": [
    "## Optimizing Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "WmmEaA8Jk9aK"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # regularization limit\n",
    "    'solver': ['liblinear', 'lbfgs', 'saga'],  # solvers that support regularization\n",
    "    'max_iter': [100, 500, 1000]  # convergence limit\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KUIGGEvlI7M"
   },
   "source": [
    "#### Steam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "aNgXsWQFemSB"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_steam' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_420610/3295292452.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgs_steam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mgs_steam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_steam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_steam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-----------STEAM LOGISTIC REGRESSION-----------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_steam' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# grid search for steam\n",
    "model = LogisticRegression(random_state=42)\n",
    "gs_steam = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "gs_steam.fit(X_train_steam, y_train_steam)\n",
    "\n",
    "print(\"-----------STEAM LOGISTIC REGRESSION-----------\")\n",
    "print(\"Best Parameters: \", gs_steam.best_params_)\n",
    "print(\"Best Score: \", gs_steam.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQd5H9wulHkK"
   },
   "source": [
    "#### Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19SLz32XhtX5",
    "outputId": "45ca1c57-e23d-44b2-a8ac-2ed466f4a4f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------YELP LOGISTIC REGRESSION-----------\n",
      "Best Parameters:  {'C': 100, 'max_iter': 100, 'solver': 'saga'}\n",
      "Best Score:  0.7775442477876107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# grid search for yelp\n",
    "model = LogisticRegression(random_state=42)\n",
    "gs_yelp = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "gs_yelp.fit(X_train_yelp, y_train_yelp)\n",
    "\n",
    "print(\"-----------YELP LOGISTIC REGRESSION-----------\")\n",
    "print(\"Best Parameters: \", gs_yelp.best_params_)\n",
    "print(\"Best Score: \", gs_yelp.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvC7rOwslGcO"
   },
   "source": [
    "#### IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OGLi99hhh1TF",
    "outputId": "08464625-46b7-4f6f-f29b-6a5d03ef48f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------IMDB LOGISTIC REGRESSION-----------\n",
      "Best Parameters:  {'C': 1, 'max_iter': 100, 'solver': 'lbfgs'}\n",
      "Best Score:  0.8881499999999999\n"
     ]
    }
   ],
   "source": [
    "# grid search for imdb\n",
    "model = LogisticRegression(random_state=42)\n",
    "gs_imdb = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "gs_imdb.fit(X_train_imdb, y_train_imdb)\n",
    "\n",
    "print(\"-----------IMDB LOGISTIC REGRESSION-----------\")\n",
    "print(\"Best Parameters: \", gs_imdb.best_params_)\n",
    "print(\"Best Score: \", gs_imdb.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LqSzGyjplFSK"
   },
   "source": [
    "#### Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2m5hJ-qJh3mG",
    "outputId": "e183806c-1eb8-4299-815e-9fa1c50e0678"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------AMAZON LOGISTIC REGRESSION-----------\n",
      "Best Parameters:  {'C': 100, 'max_iter': 100, 'solver': 'liblinear'}\n",
      "Best Score:  0.8967792021336269\n"
     ]
    }
   ],
   "source": [
    "# grid search for amazon\n",
    "model = LogisticRegression(random_state=42)\n",
    "gs_amazon = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "gs_amazon.fit(X_train_amazon, y_train_amazon)\n",
    "\n",
    "print(\"-----------AMAZON LOGISTIC REGRESSION-----------\")\n",
    "print(\"Best Parameters: \", gs_amazon.best_params_)\n",
    "print(\"Best Score: \", gs_amazon.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOtr-PEye4Nd"
   },
   "source": [
    "## Optimizing Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nMazPSiKk2sE"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'alpha': [1e-2, 1e-1, 1, 1e1, 1e2], # smoothing parameter\n",
    "    'fit_prior': [True, False], # use or ignore class priors\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hc4mwHfekL0n"
   },
   "source": [
    "#### Steam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhUB3SEhe6pK"
   },
   "outputs": [],
   "source": [
    "model = MultinomialNB(random_state=42)\n",
    "grid_search_steam = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "grid_search_steam.fit(X_train_steam, y_train_steam)\n",
    "\n",
    "print(\"-----------STEAM NB-----------\")\n",
    "print(\"Best Parameters Steam: \", grid_search_steam.best_params_)\n",
    "print(\"Best Score: \", grid_search_steam.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MnxV5TG7kQxQ"
   },
   "source": [
    "#### Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hSvAVR80jq_x",
    "outputId": "aeb1628a-4f06-4cdf-a654-14242fa5cb77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------YELP NB-----------\n",
      "Best Parameters Yelp:  {'alpha': 1, 'fit_prior': True}\n",
      "Best Score:  0.7721713021491783\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "grid_search_yelp = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "grid_search_yelp.fit(X_train_yelp, y_train_yelp)\n",
    "\n",
    "print(\"-----------YELP NB-----------\")\n",
    "print(\"Best Parameters Yelp: \", grid_search_yelp.best_params_)\n",
    "print(\"Best Score: \", grid_search_yelp.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhMzrmlwkR4i"
   },
   "source": [
    "#### IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "reOWWGJbjuIE",
    "outputId": "02cef96b-d677-479e-e2b7-b69eb359ec91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------IMDB NB-----------\n",
      "Best Parameters IMDb:  {'alpha': 1, 'fit_prior': False}\n",
      "Best Score:  0.8533999999999999\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "grid_search_imdb = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "grid_search_imdb.fit(X_train_imdb, y_train_imdb)\n",
    "\n",
    "print(\"-----------IMDB NB-----------\")\n",
    "print(\"Best Parameters IMDb: \", grid_search_imdb.best_params_)\n",
    "print(\"Best Score: \", grid_search_imdb.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evGepzw9kToo"
   },
   "source": [
    "#### Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VXzLSOYAjySN",
    "outputId": "6e81062e-7183-4304-857a-4cde7cee46e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------AMAZON NB-----------\n",
      "Best Parameters Amazon:  {'alpha': 10.0, 'fit_prior': False}\n",
      "Best Score:  0.8390612234941415\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "grid_search_amazon = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "grid_search_amazon.fit(X_train_amazon, y_train_amazon)\n",
    "\n",
    "print(\"-----------AMAZON NB-----------\")\n",
    "print(\"Best Parameters Amazon: \", grid_search_amazon.best_params_)\n",
    "print(\"Best Score: \", grid_search_amazon.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87APXvn1e55I"
   },
   "source": [
    "## Optimizing SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Tm3CL4ewlTo2"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # regularization limit\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'loss': ['squared_hinge'], # loss function\n",
    "    'dual': [True, False],\n",
    "    'max_iter': [100, 500, 1000, 5000]  # convergence limit\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "833a-pdokVVm"
   },
   "source": [
    "#### Steam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ouLZ5KrMfGuQ"
   },
   "outputs": [],
   "source": [
    "model = LinearSVC(random_state=42)\n",
    "grid_search_steam = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "grid_search_steam.fit(X_train_steam, y_train_steam)\n",
    "\n",
    "print(\"-----------STEAM SVM-----------\")\n",
    "print(\"Best Parameters Steam: \", grid_search_steam.best_params_)\n",
    "print(\"Best Score: \", grid_search_steam.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMX0GKJVkYDi"
   },
   "source": [
    "#### Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "14ov6BcQj8gS",
    "outputId": "c0be2292-6be9-4e0b-ce79-e3a990647acb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------YELP SVM-----------\n",
      "Best Parameters Yelp:  {'C': 1, 'dual': True, 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'l2'}\n",
      "Best Score:  0.7757743362831858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
      "150 fits failed out of a total of 600.\n",
      "The score on these train-test partitions for these parameters will be set to 0.0.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "150 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 317, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1214, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1046, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
     ]
    }
   ],
   "source": [
    "model = LinearSVC(random_state=42)\n",
    "grid_search_yelp = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1, error_score=0.0)\n",
    "grid_search_yelp.fit(X_train_yelp, y_train_yelp)\n",
    "\n",
    "print(\"-----------YELP SVM-----------\")\n",
    "print(\"Best Parameters Yelp: \", grid_search_yelp.best_params_)\n",
    "print(\"Best Score: \", grid_search_yelp.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSnT3WTkkY4g"
   },
   "source": [
    "#### IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A5oa3fdKkcEP",
    "outputId": "c205c696-d5c7-4b5a-e2f3-45688432de35"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
      "150 fits failed out of a total of 600.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "150 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 317, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1214, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1046, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [     nan 0.805475      nan 0.805475      nan 0.805475      nan 0.805475\n",
      "      nan 0.805475 0.500975 0.805475 0.500975 0.805475 0.500975 0.805475\n",
      " 0.500975 0.805475 0.500975 0.805475      nan 0.86275       nan 0.86275\n",
      "      nan 0.86275       nan 0.86275       nan 0.86275  0.7753   0.86275\n",
      " 0.776125 0.86275  0.776125 0.86275  0.776125 0.86275  0.776125 0.86275\n",
      "      nan 0.888775      nan 0.888775      nan 0.888775      nan 0.888775\n",
      "      nan 0.888775 0.872675 0.8888   0.8728   0.8888   0.8728   0.8888\n",
      " 0.8728   0.8888   0.8728   0.8888        nan 0.8849        nan 0.8849\n",
      "      nan 0.8849        nan 0.8849        nan 0.8849   0.88605  0.8849\n",
      " 0.885975 0.8849   0.885975 0.8849   0.885975 0.8849   0.885975 0.8849\n",
      "      nan 0.8658        nan 0.866         nan 0.866         nan 0.866\n",
      "      nan 0.866    0.861675 0.86605  0.861475 0.86605  0.861475 0.86605\n",
      " 0.861475 0.86605  0.861475 0.86605       nan 0.85425       nan 0.854125\n",
      "      nan 0.856475      nan 0.856425      nan 0.856425 0.8547   0.85645\n",
      " 0.8546   0.85645  0.8546   0.85645  0.8546   0.85645  0.8546   0.85645 ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------IMDB SVM-----------\n",
      "Best Parameters IMDb:  {'C': 0.1, 'dual': False, 'loss': 'squared_hinge', 'max_iter': 100, 'penalty': 'l2'}\n",
      "Best Score:  0.8888\n"
     ]
    }
   ],
   "source": [
    "model = LinearSVC(random_state=42)\n",
    "grid_search_imdb = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "grid_search_imdb.fit(X_train_imdb, y_train_imdb)\n",
    "\n",
    "print(\"-----------IMDB SVM-----------\")\n",
    "print(\"Best Parameters IMDb: \", grid_search_imdb.best_params_)\n",
    "print(\"Best Score: \", grid_search_imdb.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "id0myLcXkbrx"
   },
   "source": [
    "#### Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "F7JFSJ_UkiD2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
      "120 fits failed out of a total of 480.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "120 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_classes.py\", line 317, in fit\n",
      "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
      "  File \"/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1214, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/svm/_base.py\", line 1046, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/jcgarza/.local/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [       nan 0.82572021        nan 0.82572021        nan 0.82572021\n",
      "        nan 0.82572021 0.79142542 0.82571361 0.79141663 0.82571361\n",
      " 0.79141663 0.82571361 0.79141663 0.82571361        nan 0.8840341\n",
      "        nan 0.8840341         nan 0.8840341         nan 0.8840341\n",
      " 0.86650849 0.8840407  0.86681195 0.8840407  0.86681195 0.8840407\n",
      " 0.86681195 0.8840407         nan 0.89575009        nan 0.89575009\n",
      "        nan 0.89575009        nan 0.89575009 0.89359733 0.89576329\n",
      " 0.89378863 0.89576329 0.89378863 0.89576329 0.89378863 0.89576329\n",
      "        nan 0.89658569        nan 0.89658569        nan 0.89658569\n",
      "        nan 0.89658569 0.89675282 0.89655051 0.89675721 0.89655051\n",
      " 0.89675721 0.89655051 0.89675721 0.89655051        nan 0.89600737\n",
      "        nan 0.89658569        nan 0.89658569        nan 0.89658569\n",
      " 0.89660329 0.8965791  0.8965769  0.8965791  0.8965769  0.8965791\n",
      " 0.8965769  0.8965791         nan 0.85967641        nan 0.89125984\n",
      "        nan 0.89579847        nan 0.8965681  0.89658569 0.8965769\n",
      " 0.8965659  0.8965769  0.8965659  0.8965769  0.8965659  0.8965769 ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------AMAZON SVM-----------\n",
      "Best Parameters Amazon:  {'C': 1, 'dual': False, 'loss': 'squared_hinge', 'max_iter': 500, 'penalty': 'l1'}\n",
      "Best Score:  0.8967572126577895\n"
     ]
    }
   ],
   "source": [
    "model = LinearSVC(random_state=42)\n",
    "grid_search_amazon = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "grid_search_amazon.fit(X_train_amazon, y_train_amazon)\n",
    "\n",
    "print(\"-----------AMAZON SVM-----------\")\n",
    "print(\"Best Parameters Amazon: \", grid_search_amazon.best_params_)\n",
    "print(\"Best Score: \", grid_search_amazon.best_score_)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
